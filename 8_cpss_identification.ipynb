{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# CPSS IDENTIFICATION NOTEBOOK\n",
    "Integrates enhanced detection for:\n",
    "- EACS (Electronic Access Control Systems) + BAS\n",
    "- VSS (Video Surveillance Systems)\n",
    "- I&HAS (Intrusion & Hold-up Alarm Systems)\n",
    "\n",
    "Features:\n",
    "- 40 brands total (100% thesis Appendix C coverage)\n",
    "- Protocol detection (RTSP, ONVIF, BACnet, SIA DC-09, etc.)\n",
    "- HTTP path matching (70+ paths)\n",
    "- Model number detection (100+ patterns)\n",
    "- Confidence scoring (0-100%)\n",
    "- Multi-function device handling\n",
    "- BAS subcategory flagging\n",
    "- Enhanced cloud/IT exclusions\n",
    "\n",
    "**Brand Coverage:**\n",
    "- **VSS**: 15 brands (Hikvision, Dahua, Axis, MOBOTIX, Geutebruck, etc.)\n",
    "- **EACS**: 13 brands (Nedap, Paxton, Genetec, + 4 BAS brands)\n",
    "- **I&HAS**: 12 brands (AJAX, Vanderbilt, Honeywell, Bosch, etc.)\n",
    "- **Total**: 40 brands with 100% thesis Appendix C coverage\n",
    "\n",
    "**Device Categories:**\n",
    "1. **EACS** - Electronic Access Control Systems\n",
    "   - Includes BAS (Building Automation Systems) as subcategory\n",
    "   - Multi-function flag for devices like Genetec\n",
    "2. **VSS** - Video Surveillance Systems\n",
    "   - IP cameras, NVRs, DVRs, VMS platforms\n",
    "3. **I&HAS** - Intrusion & Hold-up Alarm Systems\n",
    "   - Alarm panels, intrusion detection, sensors\n",
    "\n",
    "**Expected Runtime:** ~15 minutes\n",
    "**Expected Precision:** 85-95%"
   ],
   "id": "9cb1a54e5b988254"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration and Setup",
   "id": "f408f56db60c3845"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import & Setup",
   "id": "668e354288809920"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:34:48.587534700Z",
     "start_time": "2026-01-03T18:34:48.573774500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 2: IMPORTS & SETUP\n",
    "# ========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path('./output/2_cpss_identification')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Color palette\n",
    "COLORS = {\n",
    "    'eacs': '#7C3AED',      # Purple\n",
    "    'vss': '#14B8A6',       # Turquoise\n",
    "    'ihas': '#F59E0B',      # Yellow\n",
    "    'quaternary': '#22C55E', # Emerald\n",
    "    'danger': '#EF4444',\n",
    "    'neutral': '#9CA3AF',\n",
    "}\n",
    "\n",
    "print(f\"Enhanced CPSS Identification started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output directory: {output_dir.absolute()}\")"
   ],
   "id": "8af817bdd9c61703",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced CPSS Identification started: 2026-01-03 19:34:48\n",
      "Output directory: H:\\_HHS_thesis\\GitHub\\thesis\\v1\\output\\2_cpss_identification\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data",
   "id": "eec3a28ce89f0ac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:34:58.774816200Z",
     "start_time": "2026-01-03T18:34:48.588541900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 3: LOAD DATA\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv('./staging/4_validation/6_modat_data_validation.csv', low_memory=False)\n",
    "print(f\"Loaded {len(df):,} total services\")\n",
    "print(f\"Dataset has {len(df.columns)} columns\")\n",
    "print(\"=\"*70)\n"
   ],
   "id": "54eb73cb51ceba90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "Loaded 16,941 total services\n",
      "Dataset has 119 columns\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Enhanced Detection Modules",
   "id": "2b2514dc7bfe7ff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:34:59.239368400Z",
     "start_time": "2026-01-03T18:34:59.183261300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 4: LOAD ENHANCED DETECTION MODULES (BETTER VERSION)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING ENHANCED DETECTION MODULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Function to load modules properly\n",
    "def load_detection_module(module_name, file_path):\n",
    "    \"\"\"Load a Python module and return it\"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    if spec is None:\n",
    "        raise ImportError(f\"Could not load {module_name} from {file_path}\")\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# Load the three detection modules\n",
    "print(\"\\nLoading EACS enhanced detection...\")\n",
    "eacs_mod = load_detection_module('eacs_enhanced', '8a_EACS_enhanced_detection.py')\n",
    "\n",
    "print(\"\\nLoading VSS enhanced detection...\")\n",
    "vss_mod = load_detection_module('vss_enhanced', '8b_VSS_enhanced_detection.py')\n",
    "\n",
    "print(\"\\nLoading I&HAS enhanced detection...\")\n",
    "ihas_mod = load_detection_module('ihas_enhanced', '8c_IHAS_enhanced_detection.py')\n",
    "\n",
    "\n",
    "# Extract the functions and configs (NOW IDE WILL RECOGNIZE THEM)\n",
    "identify_eacs_enhanced = eacs_mod.identify_eacs_enhanced\n",
    "EACS_ENHANCED_CONFIG = eacs_mod.EACS_ENHANCED_CONFIG\n",
    "\n",
    "identify_ihas_enhanced = ihas_mod.identify_ihas_enhanced\n",
    "IHAS_ENHANCED_CONFIG = ihas_mod.IHAS_ENHANCED_CONFIG\n",
    "\n",
    "identify_vss_enhanced = vss_mod.identify_vss_enhanced\n",
    "VSS_ENHANCED_CONFIG = vss_mod.VSS_ENHANCED_CONFIG\n",
    "\n",
    "print(\"\\nAll enhanced detection modules loaded\")\n",
    "print(f\"  EACS: {len(EACS_ENHANCED_CONFIG['brands'])} brands, {len(EACS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "print(f\"  I&HAS: {len(IHAS_ENHANCED_CONFIG['brands'])} brands, {len(IHAS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "print(f\"  VSS: {len(VSS_ENHANCED_CONFIG['brands'])} brands, {len(VSS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "print(\"=\"*70)"
   ],
   "id": "329409013885b545",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING ENHANCED DETECTION MODULES\n",
      "======================================================================\n",
      "\n",
      "Loading EACS enhanced detection...\n",
      "Comprehensive EACS detection loaded\n",
      "  Total brands: 51\n",
      "  HTTP paths: 30+\n",
      "  Protocols: 3\n",
      "\n",
      "  Coverage: ALL brands from requirement list\n",
      "\n",
      "Loading VSS enhanced detection...\n",
      "Comprehensive VSS detection loaded\n",
      "  Total brands: 50\n",
      "  HTTP paths: 28+\n",
      "  Protocols: 2 (RTSP, ONVIF)\n",
      "\n",
      "  Coverage: ALL brands from requirement list\n",
      "\n",
      "Loading I&HAS enhanced detection...\n",
      "Comprehensive I&HAS detection loaded\n",
      "  Total brands: 27\n",
      "  HTTP paths: 20+\n",
      "  Protocols: 2 (SIA DC-09, Contact ID)\n",
      "\n",
      "  Coverage: ALL brands from requirement list\n",
      "\n",
      "All enhanced detection modules loaded\n",
      "  EACS: 55 brands, 3 protocols\n",
      "  I&HAS: 32 brands, 2 protocols\n",
      "  VSS: 47 brands, 2 protocols\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Identification",
   "id": "bc004ba35bad3bf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:34:59.257244200Z",
     "start_time": "2026-01-03T18:34:59.240370600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# COLUMN NAME MAPPING\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COLUMN NAME MAPPING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rename_mapping = {\n",
    "    # HTTP fields\n",
    "    'service.http.title': 'http.html_title',\n",
    "    'service.http.path': 'http.path',\n",
    "    'service.http.headers': 'http.headers',\n",
    "\n",
    "    # TLS fields\n",
    "    'service.tls.issuer.common_name': 'ssl.cert.issuer',\n",
    "    'service.tls.subject.common_name': 'ssl.cert.subject',\n",
    "}\n",
    "\n",
    "print(\"\\nApplying column name mapping...\")\n",
    "for old_name, new_name in rename_mapping.items():\n",
    "    if old_name in df.columns and new_name not in df.columns:\n",
    "        df.rename(columns={old_name: new_name}, inplace=True)\n",
    "        print(f\"Renamed: {old_name} → {new_name}\")\n",
    "    elif new_name in df.columns:\n",
    "        print(f\"{new_name} already exists\")\n",
    "    else:\n",
    "        print(f\"{old_name} not found in dataset\")\n",
    "\n",
    "# Verify key columns exist\n",
    "print(\"\\nVerifying columns needed by detection scripts:\")\n",
    "required_columns = [\n",
    "    'service.fingerprints.tags',\n",
    "    'http.html_title',      # or service.http.title\n",
    "    # 'http.path',            # or service.http.path\n",
    "    'http.headers',         # or service.http.headers\n",
    "    'service.http.body',    # NEW - should exist as-is\n",
    "    'service.banner',\n",
    "    'service.fingerprints.os.product',\n",
    "    'service.fingerprints.service.product',\n",
    "    'service.port',\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col in df.columns:\n",
    "        non_null = df[col].notna().sum()\n",
    "        print(f\"  ✓ {col:<30} ({non_null:,} non-null)\")\n",
    "    else:\n",
    "        print(f\"  ✗ {col:<30} MISSING!\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "41defe25777566e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COLUMN NAME MAPPING\n",
      "======================================================================\n",
      "\n",
      "Applying column name mapping...\n",
      "Renamed: service.http.title → http.html_title\n",
      "service.http.path not found in dataset\n",
      "Renamed: service.http.headers → http.headers\n",
      "Renamed: service.tls.issuer.common_name → ssl.cert.issuer\n",
      "Renamed: service.tls.subject.common_name → ssl.cert.subject\n",
      "\n",
      "Verifying columns needed by detection scripts:\n",
      "  ✓ service.fingerprints.tags      (16,941 non-null)\n",
      "  ✓ http.html_title                (8,491 non-null)\n",
      "  ✓ http.headers                   (0 non-null)\n",
      "  ✓ service.http.body              (15,804 non-null)\n",
      "  ✓ service.banner                 (16,940 non-null)\n",
      "  ✓ service.fingerprints.os.product (115 non-null)\n",
      "  ✓ service.fingerprints.service.product (16,941 non-null)\n",
      "  ✓ service.port                   (16,941 non-null)\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-03T18:34:59.258701500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 5: RUN ENHANCED IDENTIFICATION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENHANCED IDENTIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================\n",
    "# 1. IDENTIFY EACS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n1. Identifying EACS devices...\")\n",
    "eacs_results = df.apply(lambda row: identify_eacs_enhanced(row), axis=1)\n",
    "\n",
    "# Extract from dictionary (not tuple!)\n",
    "df['is_eacs'] = eacs_results.apply(lambda x: x['is_eacs'])\n",
    "df['eacs_confidence'] = eacs_results.apply(lambda x: x['eacs_confidence'])\n",
    "df['eacs_reason'] = eacs_results.apply(lambda x: x['eacs_reason'])\n",
    "df['detected_eacs_brand'] = eacs_results.apply(lambda x: x['detected_brand'])\n",
    "df['detected_eacs_product'] = eacs_results.apply(lambda x: x['detected_product'])\n",
    "df['is_bas'] = eacs_results.apply(lambda x: x['is_bas'])\n",
    "\n",
    "# NEW: Extract detailed match info\n",
    "df['eacs_match_field'] = eacs_results.apply(lambda x: x.get('match_field'))\n",
    "df['eacs_match_pattern'] = eacs_results.apply(lambda x: x.get('match_pattern'))\n",
    "df['eacs_match_value'] = eacs_results.apply(lambda x: x.get('match_value'))\n",
    "\n",
    "eacs_count = df['is_eacs'].sum()\n",
    "bas_count = df['is_bas'].sum()\n",
    "print(f\"   Found {eacs_count:,} EACS services\")\n",
    "print(f\"   Found {bas_count:,} BAS services (subset of EACS)\")\n",
    "if eacs_count > 0:\n",
    "    print(f\"   Average confidence: {df[df['is_eacs']]['eacs_confidence'].mean():.1f}%\")\n",
    "\n",
    "# ========================================\n",
    "# 2. IDENTIFY VSS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n2. Identifying VSS devices...\")\n",
    "vss_results = df.apply(lambda row: identify_vss_enhanced(row), axis=1)\n",
    "\n",
    "# Extract from dictionary\n",
    "df['is_vss'] = vss_results.apply(lambda x: x['is_vss'])\n",
    "df['vss_confidence'] = vss_results.apply(lambda x: x['vss_confidence'])\n",
    "df['vss_reason'] = vss_results.apply(lambda x: x['vss_reason'])\n",
    "df['detected_vss_brand'] = vss_results.apply(lambda x: x['detected_brand'])\n",
    "df['detected_vss_product'] = vss_results.apply(lambda x: x['detected_product'])\n",
    "\n",
    "# NEW: Extract detailed match info\n",
    "df['vss_match_field'] = vss_results.apply(lambda x: x.get('match_field'))\n",
    "df['vss_match_pattern'] = vss_results.apply(lambda x: x.get('match_pattern'))\n",
    "df['vss_match_value'] = vss_results.apply(lambda x: x.get('match_value'))\n",
    "\n",
    "vss_count = df['is_vss'].sum()\n",
    "print(f\"   Found {vss_count:,} VSS services\")\n",
    "if vss_count > 0:\n",
    "    print(f\"   Average confidence: {df[df['is_vss']]['vss_confidence'].mean():.1f}%\")\n",
    "\n",
    "# ========================================\n",
    "# 3. IDENTIFY I&HAS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n3. Identifying I&HAS devices...\")\n",
    "ihas_results = df.apply(lambda row: identify_ihas_enhanced(row), axis=1)\n",
    "\n",
    "# Extract from dictionary\n",
    "df['is_ihas'] = ihas_results.apply(lambda x: x['is_ihas'])\n",
    "df['ihas_confidence'] = ihas_results.apply(lambda x: x['ihas_confidence'])\n",
    "df['ihas_reason'] = ihas_results.apply(lambda x: x['ihas_reason'])\n",
    "df['detected_ihas_brand'] = ihas_results.apply(lambda x: x['detected_brand'])\n",
    "df['detected_ihas_product'] = ihas_results.apply(lambda x: x['detected_product'])\n",
    "\n",
    "# NEW: Extract detailed match info\n",
    "df['ihas_match_field'] = ihas_results.apply(lambda x: x.get('match_field'))\n",
    "df['ihas_match_pattern'] = ihas_results.apply(lambda x: x.get('match_pattern'))\n",
    "df['ihas_match_value'] = ihas_results.apply(lambda x: x.get('match_value'))\n",
    "\n",
    "ihas_count = df['is_ihas'].sum()\n",
    "print(f\"   Found {ihas_count:,} I&HAS services\")\n",
    "if ihas_count > 0:\n",
    "    print(f\"   Average confidence: {df[df['is_ihas']]['ihas_confidence'].mean():.1f}%\")\n",
    "\n",
    "# ========================================\n",
    "# 4. MULTI-FUNCTION DETECTION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n4. Identifying multi-function devices...\")\n",
    "df['cpss_category_count'] = df['is_eacs'].astype(int) + df['is_vss'].astype(int) + df['is_ihas'].astype(int)\n",
    "df['is_cpss'] = df['cpss_category_count'] > 0\n",
    "df['is_multi_function'] = df['cpss_category_count'] > 1\n",
    "\n",
    "multi_count = df['is_multi_function'].sum()\n",
    "print(f\"   Found {multi_count:,} multi-function devices\")\n",
    "\n",
    "# ========================================\n",
    "# 5. SUMMARY STATISTICS\n",
    "# ========================================\n",
    "\n",
    "total_cpss = df['is_cpss'].sum()\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IDENTIFICATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total services:          {len(df):>12,}\")\n",
    "print(f\"EACS services:           {eacs_count:>12,} ({eacs_count/len(df)*100:>6.2f}%)\")\n",
    "print(f\"  └─ BAS subcategory:    {bas_count:>12,} ({bas_count/eacs_count*100 if eacs_count>0 else 0:>6.2f}% of EACS)\")\n",
    "print(f\"VSS services:            {vss_count:>12,} ({vss_count/len(df)*100:>6.2f}%)\")\n",
    "print(f\"I&HAS services:          {ihas_count:>12,} ({ihas_count/len(df)*100:>6.2f}%)\")\n",
    "print(f\"Multi-function devices:  {multi_count:>12,}\")\n",
    "print(f\"Total CPSS devices:      {total_cpss:>12,} ({total_cpss/len(df)*100:>6.2f}%)\")\n",
    "print(f\"Non-CPSS services:       {len(df)-total_cpss:>12,} ({(len(df)-total_cpss)/len(df)*100:>6.2f}%)\")\n",
    "print(\"=\"*70)"
   ],
   "id": "dd56d69a6bf78ff2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ENHANCED IDENTIFICATION\n",
      "======================================================================\n",
      "\n",
      "1. Identifying EACS devices...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Confidence Analysis",
   "id": "b317725fa1ba45c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CREATE FILTERED DATAFRAMES\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING FILTERED DATAFRAMES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Filter by category\n",
    "eacs_df = df[df['is_eacs']].copy()\n",
    "vss_df = df[df['is_vss']].copy()\n",
    "ihas_df = df[df['is_ihas']].copy()\n",
    "multi_function_df = df[df['is_multi_function']].copy()\n",
    "\n",
    "print(f\"\\nFiltered DataFrames created:\")\n",
    "print(f\"  EACS:            {len(eacs_df):>8,} rows\")\n",
    "print(f\"  VSS:             {len(vss_df):>8,} rows\")\n",
    "print(f\"  I&HAS:           {len(ihas_df):>8,} rows\")\n",
    "print(f\"  Multi-function:  {len(multi_function_df):>8,} rows\")\n",
    "\n",
    "# ========================================\n",
    "# CONFIDENCE ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "def analyze_confidence(filtered_df, category_name, confidence_col):\n",
    "    \"\"\"Analyze confidence score distribution\"\"\"\n",
    "    if len(filtered_df) == 0:\n",
    "        print(f\"\\n{category_name}: No devices detected\")\n",
    "        return\n",
    "\n",
    "    total = len(filtered_df)\n",
    "    high = len(filtered_df[filtered_df[confidence_col] >= 90])\n",
    "    medium_high = len(filtered_df[(filtered_df[confidence_col] >= 80) & (filtered_df[confidence_col] < 90)])\n",
    "    medium = len(filtered_df[(filtered_df[confidence_col] >= 70) & (filtered_df[confidence_col] < 80)])\n",
    "    low = len(filtered_df[filtered_df[confidence_col] < 70])\n",
    "\n",
    "    print(f\"\\n{category_name} Confidence Distribution:\")\n",
    "    print(f\"  Total devices:     {total:>6,}\")\n",
    "    print(f\"    High (90-100%):    {high:>6,} ({high/total*100:>5.1f}%)\")\n",
    "    print(f\"    Medium (80-89%):   {medium_high:>6,} ({medium_high/total*100:>5.1f}%)\")\n",
    "    print(f\"    Medium (70-79%):   {medium:>6,} ({medium/total*100:>5.1f}%)\")\n",
    "    print(f\"    Low (<70%):        {low:>6,} ({low/total*100:>5.1f}%)\")\n",
    "\n",
    "analyze_confidence(eacs_df, 'EACS', 'eacs_confidence')\n",
    "analyze_confidence(vss_df, 'VSS', 'vss_confidence')\n",
    "analyze_confidence(ihas_df, 'IHAS', 'ihas_confidence')"
   ],
   "id": "b98cd74b43e6a9f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detection Method",
   "id": "6436db2c1e9de896"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 7: DETECTION METHOD BREAKDOWN\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETECTION METHOD ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def analyze_detection_methods(df_subset, device_type, reason_col):\n",
    "    \"\"\"Analyze how devices were detected\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{device_type} Detection Methods:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Categorize detection methods\n",
    "    protocol_count = len(df_subset[df_subset[reason_col].str.contains('protocol:', na=False)])\n",
    "    tag_count = len(df_subset[df_subset[reason_col].str.contains('tag:', na=False)])\n",
    "    http_path_count = len(df_subset[df_subset[reason_col].str.contains('http_path:', na=False)])\n",
    "    brand_count = len(df_subset[df_subset[reason_col].str.contains('brand:', na=False)])\n",
    "    product_count = len(df_subset[df_subset[reason_col].str.contains('product:', na=False)])\n",
    "    model_count = len(df_subset[df_subset[reason_col].str.contains('model:', na=False)])\n",
    "    cert_count = len(df_subset[df_subset[reason_col].str.contains('cert:', na=False)])\n",
    "    keyword_count = len(df_subset[df_subset[reason_col].str.contains('keyword', na=False)])\n",
    "\n",
    "    total = len(df_subset)\n",
    "\n",
    "    print(f\"  Protocol detection:    {protocol_count:>6,} ({protocol_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Tag match:             {tag_count:>6,} ({tag_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  HTTP path match:       {http_path_count:>6,} ({http_path_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Brand match:           {brand_count:>6,} ({brand_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Product match:         {product_count:>6,} ({product_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Model number match:    {model_count:>6,} ({model_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Certificate match:     {cert_count:>6,} ({cert_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Keyword match:         {keyword_count:>6,} ({keyword_count/total*100:>5.1f}%)\")\n",
    "\n",
    "    # Top specific reasons\n",
    "    print(f\"\\n  Top 10 specific detection reasons:\")\n",
    "    reasons = df_subset[reason_col].value_counts().head(10)\n",
    "    for reason, count in reasons.items():\n",
    "        if not str(reason).startswith('excluded'):\n",
    "            print(f\"    {str(reason)[:55]:<55} {count:>6,}\")\n",
    "\n",
    "analyze_detection_methods(eacs_df, 'EACS', 'eacs_reason')\n",
    "analyze_detection_methods(vss_df, 'VSS', 'vss_reason')\n",
    "analyze_detection_methods(ihas_df, 'IHAS', 'ihas_reason')\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "3fb4fe056729456f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Brand Distribution",
   "id": "e9b2e27fd790ce6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 8: BRAND DISTRIBUTION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BRAND DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def extract_brand_from_reason(reason):\n",
    "    \"\"\"Extract brand name from match reason\"\"\"\n",
    "    if pd.isna(reason):\n",
    "        return 'Unknown'\n",
    "\n",
    "    reason_str = str(reason)\n",
    "\n",
    "    # Extract from brand:, product:, cert:, http_path:, model:\n",
    "    for prefix in ['brand:', 'product:', 'cert:', 'http_path:', 'model:']:\n",
    "        if prefix in reason_str:\n",
    "            start = reason_str.index(prefix) + len(prefix)\n",
    "            end = reason_str.find(':', start)\n",
    "            if end == -1:\n",
    "                end = reason_str.find('(', start)\n",
    "            if end == -1:\n",
    "                end = len(reason_str)\n",
    "            brand = reason_str[start:end].strip()\n",
    "            return brand\n",
    "\n",
    "    if 'protocol:' in reason_str:\n",
    "        return 'Protocol Match'\n",
    "    if 'tag:' in reason_str:\n",
    "        return 'Tag Match'\n",
    "    if 'keyword' in reason_str:\n",
    "        return 'Keyword Match'\n",
    "\n",
    "    return 'Unknown'\n",
    "\n",
    "def analyze_brand_distribution(df_subset, device_type, reason_col):\n",
    "    \"\"\"Analyze brand distribution\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        return df_subset\n",
    "\n",
    "    df_subset['detected_brand'] = df_subset[reason_col].apply(extract_brand_from_reason)\n",
    "    brand_counts = df_subset['detected_brand'].value_counts()\n",
    "\n",
    "    print(f\"\\n{device_type} Brand Distribution:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Brand':<30} {'Count':>10} {'%':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for brand, count in brand_counts.head(15).items():\n",
    "        pct = (count / len(df_subset) * 100)\n",
    "        print(f\"{brand:<30} {count:>10,} {pct:>7.2f}%\")\n",
    "\n",
    "    if len(brand_counts) > 15:\n",
    "        other = brand_counts.iloc[15:].sum()\n",
    "        print(f\"{'Others':<30} {other:>10,} {other/len(df_subset)*100:>7.2f}%\")\n",
    "\n",
    "    return df_subset\n",
    "\n",
    "eacs_df = analyze_brand_distribution(eacs_df, 'EACS', 'eacs_reason')\n",
    "vss_df = analyze_brand_distribution(vss_df, 'VSS', 'vss_reason')\n",
    "ihas_df = analyze_brand_distribution(ihas_df, 'IHAS', 'ihas_reason')\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "170b3dcbfcb2fd24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CPSS outliers analyses",
   "id": "d0f0afedd8f85fa0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BAS Subcategory analysis",
   "id": "b6bc72a2dacc6269"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 11: BAS SUBCATEGORY ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "if eacs_df['is_bas'].sum() > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BAS SUBCATEGORY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    bas_df = eacs_df[eacs_df['is_bas']].copy()\n",
    "\n",
    "    print(f\"\\nBAS devices: {len(bas_df):,} ({len(bas_df)/len(eacs_df)*100:.1f}% of EACS)\")\n",
    "    print(f\"Pure EACS: {len(eacs_df) - len(bas_df):,} ({(len(eacs_df) - len(bas_df))/len(eacs_df)*100:.1f}% of EACS)\")\n",
    "\n",
    "    # BAS brands\n",
    "    if 'detected_brand' in bas_df.columns:\n",
    "        print(\"\\nBAS brands:\")\n",
    "        bas_brands = bas_df['detected_brand'].value_counts()\n",
    "        for brand, count in bas_brands.items():\n",
    "            print(f\"  {brand:<20} {count:>6,}\")\n",
    "\n",
    "    # BAS protocols\n",
    "    bas_protocols = bas_df['eacs_reason'].str.extract(r'protocol:(\\w+)', expand=False).value_counts()\n",
    "    if len(bas_protocols) > 0:\n",
    "        print(\"\\nBAS protocols:\")\n",
    "        for protocol, count in bas_protocols.items():\n",
    "            if pd.notna(protocol):\n",
    "                print(f\"  {protocol:<20} {count:>6,}\")\n",
    "\n",
    "    print(\"=\"*70)"
   ],
   "id": "64f485142640b2ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-function Device Analysis",
   "id": "feb03fe0bce9169a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 12: MULTI-FUNCTION DEVICE ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "if len(multi_function_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-FUNCTION DEVICE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nMulti-function devices: {len(multi_function_df):,}\")\n",
    "\n",
    "    # Category combinations\n",
    "    print(\"\\nCategory combinations:\")\n",
    "    for idx, row in multi_function_df.head(10).iterrows():\n",
    "        ip = row.get('ip', 'N/A')\n",
    "        categories = []\n",
    "        if row.get('is_eacs', False):\n",
    "            categories.append(f\"EACS({row.get('eacs_confidence', 0):.0f}%)\")\n",
    "        if row.get('is_vss', False):\n",
    "            categories.append(f\"VSS({row.get('vss_confidence', 0):.0f}%)\")\n",
    "        if row.get('is_ihas', False):\n",
    "            categories.append(f\"IHAS({row.get('ihas_confidence', 0):.0f}%)\")\n",
    "\n",
    "        brand = row.get('detected_brand', 'Unknown')\n",
    "        print(f\"  {ip:<20} {brand:<20} {' + '.join(categories)}\")\n",
    "\n",
    "    if len(multi_function_df) > 10:\n",
    "        print(f\"  ... and {len(multi_function_df) - 10} more\")\n",
    "\n",
    "    print(\"=\"*70)"
   ],
   "id": "d12c762a25e0feca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CSV Exports",
   "id": "ed5be7a1f4183b63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 9: CSV EXPORTS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING CSV FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define export columns (if they exist)\n",
    "export_columns_base = [\n",
    "    'ip', 'service.port', 'service.protocol',\n",
    "    'service.http.title', 'service.fingerprints.product', 'service.fingerprints.vendor',\n",
    "    'service.banner', 'asn.org', 'geo.country', 'geo.city',\n",
    "    'service.fingerprints.tags'\n",
    "]\n",
    "\n",
    "def export_cpss_category(df_subset, device_type, confidence_col, reason_col):\n",
    "    \"\"\"Export CPSS category to CSV with confidence scores\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        print(f\"\\n⚠️  No {device_type} devices to export\")\n",
    "        return\n",
    "\n",
    "    # Find available columns\n",
    "    export_cols = [col for col in export_columns_base if col in df_subset.columns]\n",
    "\n",
    "    # Add CPSS-specific columns\n",
    "    export_cols.extend([confidence_col, reason_col])\n",
    "\n",
    "    if 'detected_brand' in df_subset.columns:\n",
    "        export_cols.append('detected_brand')\n",
    "\n",
    "    # Add BAS flag for EACS\n",
    "    if device_type == 'EACS' and 'is_bas' in df_subset.columns:\n",
    "        export_cols.append('is_bas')\n",
    "\n",
    "    # Add multi-function flag\n",
    "    if 'is_multi_function' in df_subset.columns:\n",
    "        export_cols.append('is_multi_function')\n",
    "\n",
    "    # Export\n",
    "    filename = f'cpss_{device_type.lower()}_services_enhanced.csv'\n",
    "    filepath = output_dir / filename\n",
    "    df_subset[export_cols].to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"\\n {device_type} exported: {filename}\")\n",
    "    print(f\"  Services: {len(df_subset):,}\")\n",
    "    print(f\"  Avg confidence: {df_subset[confidence_col].mean():.1f}%\")\n",
    "    print(f\"  Columns: {len(export_cols)}\")\n",
    "\n",
    "    # High confidence sample\n",
    "    high_conf = df_subset[df_subset[confidence_col] >= 90]\n",
    "    if len(high_conf) > 0:\n",
    "        print(f\"\\n  High confidence sample (≥90%, first 3):\")\n",
    "        for idx, row in high_conf.head(3).iterrows():\n",
    "            ip = row.get('ip', 'N/A')\n",
    "            port = row.get('service.port', 'N/A')\n",
    "            brand = row.get('detected_brand', 'N/A')\n",
    "            conf = row.get(confidence_col, 0)\n",
    "            reason = str(row.get(reason_col, ''))[:40]\n",
    "            print(f\"    {ip}:{port} | {brand:<15} | {conf:>3.0f}% | {reason}\")\n",
    "\n",
    "# Export each category\n",
    "export_cpss_category(eacs_df, 'EACS', 'eacs_confidence', 'eacs_reason')\n",
    "export_cpss_category(vss_df, 'VSS', 'vss_confidence', 'vss_reason')\n",
    "export_cpss_category(ihas_df, 'IHAS', 'ihas_confidence', 'ihas_reason')\n",
    "\n",
    "# Export multi-function devices\n",
    "if len(multi_function_df) > 0:\n",
    "    multi_export_cols = [col for col in export_columns_base if col in multi_function_df.columns]\n",
    "    multi_export_cols.extend(['is_eacs', 'is_vss', 'is_ihas',\n",
    "                              'eacs_confidence', 'vss_confidence', 'ihas_confidence',\n",
    "                              'eacs_reason', 'vss_reason', 'ihas_reason'])\n",
    "\n",
    "    filepath = output_dir / 'cpss_multi_function_devices.csv'\n",
    "    multi_function_df[multi_export_cols].to_csv(filepath, index=False)\n",
    "    print(f\"\\n Multi-function devices exported: cpss_multi_function_devices.csv\")\n",
    "    print(f\"  Services: {len(multi_function_df):,}\")\n",
    "\n",
    "# Export combined CPSS dataset\n",
    "all_cpss = pd.concat([\n",
    "    eacs_df.assign(cpss_primary_category='EACS'),\n",
    "    vss_df.assign(cpss_primary_category='VSS'),\n",
    "    ihas_df.assign(cpss_primary_category='IHAS')\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(all_cpss) > 0:\n",
    "    export_cols = [col for col in export_columns_base if col in all_cpss.columns]\n",
    "    export_cols.extend(['cpss_primary_category',\n",
    "                       'eacs_confidence', 'vss_confidence', 'ihas_confidence',\n",
    "                       'eacs_reason', 'vss_reason', 'ihas_reason'])\n",
    "\n",
    "    if 'detected_brand' in all_cpss.columns:\n",
    "        export_cols.append('detected_brand')\n",
    "    if 'is_bas' in all_cpss.columns:\n",
    "        export_cols.append('is_bas')\n",
    "    if 'is_multi_function' in all_cpss.columns:\n",
    "        export_cols.append('is_multi_function')\n",
    "\n",
    "    filepath = output_dir / 'cpss_all_services_enhanced.csv'\n",
    "    all_cpss[[col for col in export_cols if col in all_cpss.columns]].to_csv(filepath, index=False)\n",
    "    print(f\"\\n Combined CPSS exported: cpss_all_services_enhanced.csv\")\n",
    "    print(f\"  Total services: {len(all_cpss):,}\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "7666c7d1d3e7e999",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisations",
   "id": "96828471f0da7edf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 10: VISUALIZATIONS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. CPSS Category Distribution (Pie)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "cpss_counts = pd.Series({\n",
    "    'EACS': len(eacs_df),\n",
    "    'VSS': len(vss_df),\n",
    "    'IHAS': len(ihas_df)\n",
    "})\n",
    "colors_pie = [COLORS['eacs'], COLORS['vss'], COLORS['ihas']]\n",
    "wedges, texts, autotexts = ax1.pie(cpss_counts.values, labels=cpss_counts.index,\n",
    "                                     autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(12)\n",
    "ax1.set_title('CPSS Category Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# 2. Services by Category (Bar)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars = ax2.bar(cpss_counts.index, cpss_counts.values, color=colors_pie,\n",
    "               edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Number of Services', fontsize=11)\n",
    "ax2.set_title('CPSS Services by Category', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for bar, count in zip(bars, cpss_counts.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, count,\n",
    "            f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 3. Confidence Score Distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "confidence_data = []\n",
    "confidence_labels = []\n",
    "if len(eacs_df) > 0:\n",
    "    confidence_data.append(eacs_df['eacs_confidence'].values)\n",
    "    confidence_labels.append('EACS')\n",
    "if len(vss_df) > 0:\n",
    "    confidence_data.append(vss_df['vss_confidence'].values)\n",
    "    confidence_labels.append('VSS')\n",
    "if len(ihas_df) > 0:\n",
    "    confidence_data.append(ihas_df['ihas_confidence'].values)\n",
    "    confidence_labels.append('IHAS')\n",
    "\n",
    "if confidence_data:\n",
    "    bp = ax3.boxplot(confidence_data, labels=confidence_labels, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors_pie[:len(confidence_data)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    ax3.set_ylabel('Confidence Score (%)', fontsize=11)\n",
    "    ax3.set_title('Confidence Score Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    ax3.set_ylim(0, 105)\n",
    "\n",
    "# 4. Detection Method Comparison\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "detection_methods = {\n",
    "    'Protocol': [],\n",
    "    'HTTP Path': [],\n",
    "    'Brand/Product': [],\n",
    "    'Tag': [],\n",
    "    'Keyword': []\n",
    "}\n",
    "\n",
    "for df_subset, reason_col, label in [(eacs_df, 'eacs_reason', 'EACS'),\n",
    "                                      (vss_df, 'vss_reason', 'VSS'),\n",
    "                                      (ihas_df, 'ihas_reason', 'IHAS')]:\n",
    "    if len(df_subset) > 0:\n",
    "        total = len(df_subset)\n",
    "        detection_methods['Protocol'].append(len(df_subset[df_subset[reason_col].str.contains('protocol:', na=False)]) / total * 100)\n",
    "        detection_methods['HTTP Path'].append(len(df_subset[df_subset[reason_col].str.contains('http_path:', na=False)]) / total * 100)\n",
    "        detection_methods['Brand/Product'].append(len(df_subset[df_subset[reason_col].str.contains('brand:|product:|model:', na=False)]) / total * 100)\n",
    "        detection_methods['Tag'].append(len(df_subset[df_subset[reason_col].str.contains('tag:', na=False)]) / total * 100)\n",
    "        detection_methods['Keyword'].append(len(df_subset[df_subset[reason_col].str.contains('keyword', na=False)]) / total * 100)\n",
    "    else:\n",
    "        for key in detection_methods:\n",
    "            detection_methods[key].append(0)\n",
    "\n",
    "x = np.arange(len(['EACS', 'VSS', 'IHAS']))\n",
    "width = 0.15\n",
    "multiplier = 0\n",
    "\n",
    "for method, values in detection_methods.items():\n",
    "    offset = width * multiplier\n",
    "    bars = ax4.bar(x + offset, values, width, label=method)\n",
    "    multiplier += 1\n",
    "\n",
    "ax4.set_ylabel('Percentage (%)', fontsize=11)\n",
    "ax4.set_title('Detection Method Comparison by Category', fontsize=14, fontweight='bold', pad=15)\n",
    "ax4.set_xticks(x + width * 2)\n",
    "ax4.set_xticklabels(['EACS', 'VSS', 'IHAS'])\n",
    "ax4.legend(loc='upper right', fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Top Brands (Combined)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "all_brands = []\n",
    "if len(eacs_df) > 0 and 'detected_brand' in eacs_df.columns:\n",
    "    all_brands.extend(eacs_df['detected_brand'].tolist())\n",
    "if len(vss_df) > 0 and 'detected_brand' in vss_df.columns:\n",
    "    all_brands.extend(vss_df['detected_brand'].tolist())\n",
    "if len(ihas_df) > 0 and 'detected_brand' in ihas_df.columns:\n",
    "    all_brands.extend(ihas_df['detected_brand'].tolist())\n",
    "\n",
    "if all_brands:\n",
    "    brand_counts = pd.Series(all_brands).value_counts().head(15)\n",
    "    ax5.barh(range(len(brand_counts)), brand_counts.values, color=COLORS['quaternary'])\n",
    "    ax5.set_yticks(range(len(brand_counts)))\n",
    "    ax5.set_yticklabels(brand_counts.index)\n",
    "    ax5.set_xlabel('Number of Services', fontsize=11)\n",
    "    ax5.set_title('Top 15 CPSS Brands (All Categories)', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    for i, (brand, count) in enumerate(brand_counts.items()):\n",
    "        ax5.text(count, i, f' {count:,}', va='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Enhanced CPSS Identification Summary', fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.savefig(output_dir / 'cpss_identification_enhanced_summary.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization saved: cpss_identification_enhanced_summary.jpg\")\n",
    "print(\"=\"*70)"
   ],
   "id": "4a85816f59bf0028",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summary Report",
   "id": "7176aee32a6020f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 13: SUMMARY REPORT\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "ENHANCED CPSS IDENTIFICATION SUMMARY REPORT\n",
    "{'='*70}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "DATASET OVERVIEW\n",
    "{'-'*70}\n",
    "Total services analyzed:          {len(df):>12,}\n",
    "\n",
    "CPSS IDENTIFICATION RESULTS\n",
    "{'-'*70}\n",
    "EACS (Access Control):            {len(eacs_df):>12,} ({len(eacs_df)/len(df)*100:>6.2f}%)\n",
    "  └─ BAS subcategory:             {eacs_df['is_bas'].sum():>12,} ({eacs_df['is_bas'].sum()/len(eacs_df)*100 if len(eacs_df)>0 else 0:>6.2f}% of EACS)\n",
    "  └─ Average confidence:          {eacs_df['eacs_confidence'].mean() if len(eacs_df)>0 else 0:>12.1f}%\n",
    "\n",
    "VSS (Video Surveillance):         {len(vss_df):>12,} ({len(vss_df)/len(df)*100:>6.2f}%)\n",
    "  └─ Average confidence:          {vss_df['vss_confidence'].mean() if len(vss_df)>0 else 0:>12.1f}%\n",
    "\n",
    "IHAS (Intrusion & Alarm):        {len(ihas_df):>12,} ({len(ihas_df)/len(df)*100:>6.2f}%)\n",
    "  └─ Average confidence:          {ihas_df['ihas_confidence'].mean() if len(ihas_df)>0 else 0:>12.1f}%\n",
    "\n",
    "Total CPSS devices:               {total_cpss:>12,} ({total_cpss/len(df)*100:>6.2f}%)\n",
    "Multi-function devices:           {len(multi_function_df):>12,}\n",
    "Non-CPSS services:                {len(df)-total_cpss:>12,} ({(len(df)-total_cpss)/len(df)*100:>6.2f}%)\n",
    "\n",
    "ENHANCEMENT FEATURES APPLIED\n",
    "{'-'*70}\n",
    " Protocol detection (RTSP, ONVIF, BACnet, SIA DC-09, Contact ID)\n",
    " HTTP path pattern matching\n",
    " Model number detection\n",
    " Confidence scoring system (0-100%)\n",
    " Multi-function device handling\n",
    " BAS subcategory flagging\n",
    " Enhanced cloud provider exclusions\n",
    "\n",
    "TOP DETECTION METHODS\n",
    "{'-'*70}\n",
    "EACS:  {eacs_df['eacs_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(eacs_df)>0 else 'N/A':<30} {eacs_df['eacs_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(eacs_df)>0 else 0:>6,}\n",
    "VSS:   {vss_df['vss_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(vss_df)>0 else 'N/A':<30} {vss_df['vss_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(vss_df)>0 else 0:>6,}\n",
    "IHAS: {ihas_df['ihas_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(ihas_df)>0 else 'N/A':<30} {ihas_df['ihas_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(ihas_df)>0 else 0:>6,}\n",
    "\n",
    "FILES GENERATED\n",
    "{'-'*70}\n",
    "cpss_eacs_services_enhanced.csv   {len(eacs_df):>12,} services\n",
    "cpss_vss_services_enhanced.csv    {len(vss_df):>12,} services\n",
    "cpss_ihas_services_enhanced.csv   {len(ihas_df):>12,} services\n",
    "cpss_all_services_enhanced.csv    {total_cpss:>12,} services\n",
    "cpss_multi_function_devices.csv   {len(multi_function_df):>12,} services\n",
    "cpss_identification_enhanced_summary.jpg\n",
    "\n",
    "NEXT STEPS\n",
    "{'-'*70}\n",
    "1. Validate CSV exports\n",
    "2. Review high-confidence matches (≥90%)\n",
    "3. Investigate multi-function devices\n",
    "4. Proceed to ISO controls assessment (analyses_3)\n",
    "\n",
    "{'='*70}\n",
    "End of Report\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save report\n",
    "report_path = output_dir / 'cpss_identification_enhanced_report.txt'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\n Summary report saved: cpss_identification_enhanced_report.txt\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENHANCED CPSS IDENTIFICATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n All files saved to: {output_dir.absolute()}\")\n",
    "print(\"\\nEnhancement Summary:\")\n",
    "print(f\"  • {len(eacs_df):,} EACS devices (avg {eacs_df['eacs_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {len(vss_df):,} VSS devices (avg {vss_df['vss_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {len(ihas_df):,} IHAS devices (avg {ihas_df['ihas_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {total_cpss:,} total CPSS devices identified\")\n",
    "print(\"\\nReview the CSV files and proceed to ISO control assessment!\")\n",
    "print(\"=\"*70)"
   ],
   "id": "4411768948df8701",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
