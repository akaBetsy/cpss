{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# CPSS IDENTIFICATION NOTEBOOK\n",
    "Integrates enhanced detection for:\n",
    "- EACS (Electronic Access Control Systems) + BAS\n",
    "- VSS (Video Surveillance Systems)\n",
    "- I&HAS (Intrusion & Hold-up Alarm Systems)\n",
    "\n",
    "Features:\n",
    "- 40 brands total (100% thesis Appendix C coverage)\n",
    "- Protocol detection (RTSP, ONVIF, BACnet, SIA DC-09, etc.)\n",
    "- HTTP path matching (70+ paths)\n",
    "- Model number detection (100+ patterns)\n",
    "- Confidence scoring (0-100%)\n",
    "- Multi-function device handling\n",
    "- BAS subcategory flagging\n",
    "- Enhanced cloud/IT exclusions\n",
    "\n",
    "**Brand Coverage:**\n",
    "- **VSS**: 15 brands (Hikvision, Dahua, Axis, MOBOTIX, Geutebruck, etc.)\n",
    "- **EACS**: 13 brands (Nedap, Paxton, Genetec, + 4 BAS brands)\n",
    "- **I&HAS**: 12 brands (AJAX, Vanderbilt, Honeywell, Bosch, etc.)\n",
    "- **Total**: 40 brands with 100% thesis Appendix C coverage\n",
    "\n",
    "**Device Categories:**\n",
    "1. **EACS** - Electronic Access Control Systems\n",
    "   - Includes BAS (Building Automation Systems) as subcategory\n",
    "   - Multi-function flag for devices like Genetec\n",
    "2. **VSS** - Video Surveillance Systems\n",
    "   - IP cameras, NVRs, DVRs, VMS platforms\n",
    "3. **I&HAS** - Intrusion & Hold-up Alarm Systems\n",
    "   - Alarm panels, intrusion detection, sensors\n",
    "\n",
    "**Expected Runtime:** ~15 minutes\n",
    "**Expected Precision:** 85-95%"
   ],
   "id": "9cb1a54e5b988254"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration and Setup",
   "id": "f408f56db60c3845"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import & Setup",
   "id": "668e354288809920"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:06:47.255959Z",
     "start_time": "2026-01-01T14:06:47.156767900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 2: IMPORTS & SETUP\n",
    "# ========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path('./output/2_cpss_identification')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Color palette\n",
    "COLORS = {\n",
    "    'eacs': '#7C3AED',      # Purple\n",
    "    'vss': '#14B8A6',       # Turquoise\n",
    "    'ihas': '#F59E0B',      # Yellow\n",
    "    'quaternary': '#22C55E', # Emerald\n",
    "    'danger': '#EF4444',\n",
    "    'neutral': '#9CA3AF',\n",
    "}\n",
    "\n",
    "print(f\"Enhanced CPSS Identification started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output directory: {output_dir.absolute()}\")"
   ],
   "id": "8af817bdd9c61703",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced CPSS Identification started: 2026-01-01 15:06:47\n",
      "Output directory: H:\\_HHS_thesis\\GitHub\\thesis\\v1\\output\\2_cpss_identification\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data",
   "id": "eec3a28ce89f0ac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:07:02.555107800Z",
     "start_time": "2026-01-01T14:06:47.273648800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 3: LOAD DATA\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv('./staging/3_prepare_analyses/modat_service_all.csv', low_memory=False)\n",
    "print(f\"Loaded {len(df):,} total services\")\n",
    "print(f\"Dataset has {len(df.columns)} columns\")\n",
    "print(\"=\"*70)\n"
   ],
   "id": "54eb73cb51ceba90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "Loaded 271,255 total services\n",
      "Dataset has 175 columns\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Enhanced Detection Modules",
   "id": "2b2514dc7bfe7ff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:07:02.640916200Z",
     "start_time": "2026-01-01T14:07:02.600786600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 4: LOAD ENHANCED DETECTION MODULES (BETTER VERSION)\n",
    "# ========================================\n",
    "# Dit lost de \"unresolved reference\" warnings op\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING ENHANCED DETECTION MODULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Function to load modules properly\n",
    "def load_detection_module(module_name, file_path):\n",
    "    \"\"\"Load a Python module and return it\"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    if spec is None:\n",
    "        raise ImportError(f\"Could not load {module_name} from {file_path}\")\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# Load the three detection modules\n",
    "print(\"\\nLoading EACS enhanced detection...\")\n",
    "eacs_mod = load_detection_module('eacs_enhanced', '6__EACS_enhanced_detection.py')\n",
    "\n",
    "print(\"\\nLoading I&HAS enhanced detection...\")\n",
    "ihas_mod = load_detection_module('ihas_enhanced', '6__IHAS_enhanced_detection.py')\n",
    "\n",
    "print(\"\\nLoading VSS enhanced detection...\")\n",
    "vss_mod = load_detection_module('vss_enhanced', '6__VSS_enhanced_detection.py')\n",
    "\n",
    "# Extract the functions and configs (NOW IDE WILL RECOGNIZE THEM)\n",
    "identify_eacs_enhanced = eacs_mod.identify_eacs_enhanced\n",
    "EACS_ENHANCED_CONFIG = eacs_mod.EACS_ENHANCED_CONFIG\n",
    "\n",
    "identify_ihas_enhanced = ihas_mod.identify_ihas_enhanced\n",
    "IHAS_ENHANCED_CONFIG = ihas_mod.IHAS_ENHANCED_CONFIG\n",
    "\n",
    "identify_vss_enhanced = vss_mod.identify_vss_enhanced\n",
    "VSS_ENHANCED_CONFIG = vss_mod.VSS_ENHANCED_CONFIG\n",
    "\n",
    "print(\"\\nAll enhanced detection modules loaded\")\n",
    "print(f\"  EACS: {len(EACS_ENHANCED_CONFIG['brands'])} brands, {len(EACS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "print(f\"  I&HAS: {len(IHAS_ENHANCED_CONFIG['brands'])} brands, {len(IHAS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "print(f\"  VSS: {len(VSS_ENHANCED_CONFIG['brands'])} brands, {len(VSS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "print(\"=\"*70)"
   ],
   "id": "329409013885b545",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING ENHANCED DETECTION MODULES\n",
      "======================================================================\n",
      "\n",
      "Loading EACS enhanced detection...\n",
      "Enhanced EACS detection loaded (STRICT VERSION)\n",
      "  Brands: 13\n",
      "  HTTP paths: 35+ (specific only)\n",
      "  Protocols: 3 (with banner confirmation)\n",
      "\n",
      "  CHANGES FROM PREVIOUS:\n",
      "  - NO single-word matches (edge, evo, space)\n",
      "  - Require brand + product combination\n",
      "  - Enhanced exclusions for false positives\n",
      "  - Port detection requires banner confirmation\n",
      "\n",
      "  Expected: Lower recall, MUCH higher precision\n",
      "  Goal: Zero false positives\n",
      "\n",
      "Loading I&HAS enhanced detection...\n",
      "Enhanced I&HAS detection loaded (STRICT VERSION)\n",
      "  Brands: 10\n",
      "  HTTP paths: 25+ (product-specific)\n",
      "  Protocols: 2 (with banner confirmation)\n",
      "\n",
      "  CHANGES:\n",
      "  - NO generic terms (alarm, security, sensor)\n",
      "  - Require brand + specific product\n",
      "  - Detailed match reporting in output\n",
      "  - Enhanced exclusions\n",
      "\n",
      "  Expected: Much lower count, high precision\n",
      "\n",
      "Loading VSS enhanced detection...\n",
      "Enhanced VSS detection loaded (STRICT VERSION)\n",
      "  Brands: 11\n",
      "  HTTP paths: 35+ (camera/NVR specific)\n",
      "  Protocols: 2 (RTSP, ONVIF)\n",
      "\n",
      "  CHANGES:\n",
      "  - Require brand + product context\n",
      "  - Enhanced cloud/IT camera exclusions\n",
      "  - Detailed match reporting\n",
      "\n",
      "  Expected: High precision, validated results\n",
      "\n",
      "All enhanced detection modules loaded\n",
      "  EACS: 11 brands, 3 protocols\n",
      "  I&HAS: 10 brands, 2 protocols\n",
      "  VSS: 11 brands, 2 protocols\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Identification",
   "id": "bc004ba35bad3bf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:10:41.902442400Z",
     "start_time": "2026-01-01T14:09:30.162397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 5: RUN ENHANCED IDENTIFICATION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING ENHANCED CPSS IDENTIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\"\"1. Identifying EACS devices...\")\n",
    "eacs_results = df.apply(lambda row: identify_eacs_enhanced(row), axis=1)\n",
    "df['is_eacs'] = eacs_results.apply(lambda x: x[0])\n",
    "df['eacs_confidence'] = eacs_results.apply(lambda x: x[1])\n",
    "df['eacs_reason'] = eacs_results.apply(lambda x: x[2])\n",
    "\n",
    "# Add BAS subcategory flag\n",
    "df['is_bas'] = df['eacs_reason'].str.contains('bacnet|lon|modbus|desigo|metasys|struxureware|enteliweb', case=False, na=False)\n",
    "\n",
    "print(\"EACS identification complete\")\n",
    "\n",
    "print(\"\\n2. Identifying VSS devices...\")\n",
    "vss_results = df.apply(lambda row: identify_vss_enhanced(row), axis=1)\n",
    "df['is_vss'] = vss_results.apply(lambda x: x[0])\n",
    "df['vss_confidence'] = vss_results.apply(lambda x: x[1])\n",
    "df['vss_reason'] = vss_results.apply(lambda x: x[2])\n",
    "\n",
    "print(\"VSS identification complete\")\n",
    "\n",
    "print(\"\\n3. Identifying IHAS devices...\")\n",
    "ihas_results = df.apply(lambda row: identify_ihas_enhanced(row), axis=1)\n",
    "df['is_ihas'] = ihas_results.apply(lambda x: x[0])\n",
    "df['ihas_confidence'] = ihas_results.apply(lambda x: x[1])\n",
    "df['ihas_reason'] = ihas_results.apply(lambda x: x[2])\n",
    "\n",
    "print(\"IHAS identification complete\")\n",
    "\n",
    "# Extract datasets\n",
    "eacs_df = df[df['is_eacs']].copy()\n",
    "vss_df = df[df['is_vss']].copy()\n",
    "ihas_df = df[df['is_ihas']].copy()\n",
    "\n",
    "# Flag multi-function devices\n",
    "df['is_multi_function'] = (df['is_eacs'].astype(int) + df['is_vss'].astype(int) + df['is_ihas'].astype(int)) > 1\n",
    "\n",
    "multi_function_df = df[df['is_multi_function']].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IDENTIFICATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal services in dataset: {len(df):,}\")\n",
    "print(f\"\\nEACS identified:   {len(eacs_df):>8,} ({len(eacs_df)/len(df)*100:>5.2f}%)\")\n",
    "print(f\"  └─ BAS devices:  {eacs_df['is_bas'].sum():>8,} ({eacs_df['is_bas'].sum()/len(eacs_df)*100 if len(eacs_df)>0 else 0:>5.2f}% of EACS)\")\n",
    "print(f\"VSS identified:    {len(vss_df):>8,} ({len(vss_df)/len(df)*100:>5.2f}%)\")\n",
    "print(f\"IHAS identified:  {len(ihas_df):>8,} ({len(ihas_df)/len(df)*100:>5.2f}%)\")\n",
    "\n",
    "total_cpss = len(df[(df['is_eacs']) | (df['is_vss']) | (df['is_ihas'])])\n",
    "print(f\"\\nTotal CPSS:        {total_cpss:>8,} ({total_cpss/len(df)*100:>5.2f}%)\")\n",
    "print(f\"Multi-function:    {len(multi_function_df):>8,}\")\n",
    "\n",
    "if len(multi_function_df) > 0:\n",
    "    print(f\"\\nMulti-function breakdown:\")\n",
    "    overlap_eacs_vss = len(df[df['is_eacs'] & df['is_vss']])\n",
    "    overlap_eacs_ihas = len(df[df['is_eacs'] & df['is_ihas']])\n",
    "    overlap_vss_ihas = len(df[df['is_vss'] & df['is_ihas']])\n",
    "\n",
    "    if overlap_eacs_vss > 0:\n",
    "        print(f\"  EACS + VSS:     {overlap_eacs_vss:>8,}\")\n",
    "    if overlap_eacs_ihas > 0:\n",
    "        print(f\"  EACS + IHAS:   {overlap_eacs_ihas:>8,}\")\n",
    "    if overlap_vss_ihas > 0:\n",
    "        print(f\"  VSS + IHAS:    {overlap_vss_ihas:>8,}\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "dd56d69a6bf78ff2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING ENHANCED CPSS IDENTIFICATION\n",
      "======================================================================\n",
      "\n",
      "1. Identifying EACS devices...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m1. Identifying EACS devices...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     10\u001B[39m eacs_results = df.apply(\u001B[38;5;28;01mlambda\u001B[39;00m row: identify_eacs_enhanced(row), axis=\u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mis_eacs\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43meacs_results\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33meacs_confidence\u001B[39m\u001B[33m'\u001B[39m] = eacs_results.apply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[32m1\u001B[39m])\n\u001B[32m     13\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33meacs_reason\u001B[39m\u001B[33m'\u001B[39m] = eacs_results.apply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[32m2\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\series.py:4935\u001B[39m, in \u001B[36mSeries.apply\u001B[39m\u001B[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[39m\n\u001B[32m   4800\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply\u001B[39m(\n\u001B[32m   4801\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4802\u001B[39m     func: AggFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4807\u001B[39m     **kwargs,\n\u001B[32m   4808\u001B[39m ) -> DataFrame | Series:\n\u001B[32m   4809\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4810\u001B[39m \u001B[33;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[32m   4811\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4926\u001B[39m \u001B[33;03m    dtype: float64\u001B[39;00m\n\u001B[32m   4927\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   4928\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4929\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4930\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4931\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4932\u001B[39m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m=\u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4933\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4934\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m4935\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\apply.py:1422\u001B[39m, in \u001B[36mSeriesApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1419\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_compat()\n\u001B[32m   1421\u001B[39m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1422\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\apply.py:1502\u001B[39m, in \u001B[36mSeriesApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1496\u001B[39m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[32m   1497\u001B[39m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[32m   1498\u001B[39m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[32m   1499\u001B[39m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[32m   1500\u001B[39m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[32m   1501\u001B[39m action = \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj.dtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1502\u001B[39m mapped = \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1503\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[32m   1504\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1506\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[32m0\u001B[39m], ABCSeries):\n\u001B[32m   1507\u001B[39m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[32m   1508\u001B[39m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[32m   1509\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj._constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index=obj.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\base.py:925\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    922\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    923\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m925\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/lib.pyx:2999\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36m<lambda>\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m1. Identifying EACS devices...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     10\u001B[39m eacs_results = df.apply(\u001B[38;5;28;01mlambda\u001B[39;00m row: identify_eacs_enhanced(row), axis=\u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mis_eacs\u001B[39m\u001B[33m'\u001B[39m] = eacs_results.apply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[32m     12\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33meacs_confidence\u001B[39m\u001B[33m'\u001B[39m] = eacs_results.apply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[32m1\u001B[39m])\n\u001B[32m     13\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33meacs_reason\u001B[39m\u001B[33m'\u001B[39m] = eacs_results.apply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[32m2\u001B[39m])\n",
      "\u001B[31mKeyError\u001B[39m: 0"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Confidence Analysis",
   "id": "b317725fa1ba45c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 6: CONFIDENCE ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIDENCE SCORE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def analyze_confidence(df_subset, device_type, confidence_col):\n",
    "    \"\"\"Analyze confidence score distribution\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{device_type} Confidence Scores:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    avg_conf = df_subset[confidence_col].mean()\n",
    "    min_conf = df_subset[confidence_col].min()\n",
    "    max_conf = df_subset[confidence_col].max()\n",
    "\n",
    "    print(f\"  Average:  {avg_conf:.1f}%\")\n",
    "    print(f\"  Range:    {min_conf:.0f}% - {max_conf:.0f}%\")\n",
    "\n",
    "    # Confidence bands\n",
    "    very_high = len(df_subset[df_subset[confidence_col] >= 90])\n",
    "    high = len(df_subset[(df_subset[confidence_col] >= 80) & (df_subset[confidence_col] < 90)])\n",
    "    medium = len(df_subset[(df_subset[confidence_col] >= 70) & (df_subset[confidence_col] < 80)])\n",
    "    low = len(df_subset[df_subset[confidence_col] < 70])\n",
    "\n",
    "    total = len(df_subset)\n",
    "\n",
    "    print(f\"\\n  Confidence bands:\")\n",
    "    print(f\"    Very High (≥90%):  {very_high:>6,} ({very_high/total*100:>5.1f}%)\")\n",
    "    print(f\"    High (80-89%):     {high:>6,} ({high/total*100:>5.1f}%)\")\n",
    "    print(f\"    Medium (70-79%):   {medium:>6,} ({medium/total*100:>5.1f}%)\")\n",
    "    print(f\"    Low (<70%):        {low:>6,} ({low/total*100:>5.1f}%)\")\n",
    "\n",
    "analyze_confidence(eacs_df, 'EACS', 'eacs_confidence')\n",
    "analyze_confidence(vss_df, 'VSS', 'vss_confidence')\n",
    "analyze_confidence(ihas_df, 'IHAS', 'ihas_confidence')\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "b98cd74b43e6a9f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detection Method",
   "id": "6436db2c1e9de896"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 7: DETECTION METHOD BREAKDOWN\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETECTION METHOD ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def analyze_detection_methods(df_subset, device_type, reason_col):\n",
    "    \"\"\"Analyze how devices were detected\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{device_type} Detection Methods:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Categorize detection methods\n",
    "    protocol_count = len(df_subset[df_subset[reason_col].str.contains('protocol:', na=False)])\n",
    "    tag_count = len(df_subset[df_subset[reason_col].str.contains('tag:', na=False)])\n",
    "    http_path_count = len(df_subset[df_subset[reason_col].str.contains('http_path:', na=False)])\n",
    "    brand_count = len(df_subset[df_subset[reason_col].str.contains('brand:', na=False)])\n",
    "    product_count = len(df_subset[df_subset[reason_col].str.contains('product:', na=False)])\n",
    "    model_count = len(df_subset[df_subset[reason_col].str.contains('model:', na=False)])\n",
    "    cert_count = len(df_subset[df_subset[reason_col].str.contains('cert:', na=False)])\n",
    "    keyword_count = len(df_subset[df_subset[reason_col].str.contains('keyword', na=False)])\n",
    "\n",
    "    total = len(df_subset)\n",
    "\n",
    "    print(f\"  Protocol detection:    {protocol_count:>6,} ({protocol_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Tag match:             {tag_count:>6,} ({tag_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  HTTP path match:       {http_path_count:>6,} ({http_path_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Brand match:           {brand_count:>6,} ({brand_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Product match:         {product_count:>6,} ({product_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Model number match:    {model_count:>6,} ({model_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Certificate match:     {cert_count:>6,} ({cert_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Keyword match:         {keyword_count:>6,} ({keyword_count/total*100:>5.1f}%)\")\n",
    "\n",
    "    # Top specific reasons\n",
    "    print(f\"\\n  Top 10 specific detection reasons:\")\n",
    "    reasons = df_subset[reason_col].value_counts().head(10)\n",
    "    for reason, count in reasons.items():\n",
    "        if not str(reason).startswith('excluded'):\n",
    "            print(f\"    {str(reason)[:55]:<55} {count:>6,}\")\n",
    "\n",
    "analyze_detection_methods(eacs_df, 'EACS', 'eacs_reason')\n",
    "analyze_detection_methods(vss_df, 'VSS', 'vss_reason')\n",
    "analyze_detection_methods(ihas_df, 'IHAS', 'ihas_reason')\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "3fb4fe056729456f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Brand Distribution",
   "id": "e9b2e27fd790ce6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 8: BRAND DISTRIBUTION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BRAND DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def extract_brand_from_reason(reason):\n",
    "    \"\"\"Extract brand name from match reason\"\"\"\n",
    "    if pd.isna(reason):\n",
    "        return 'Unknown'\n",
    "\n",
    "    reason_str = str(reason)\n",
    "\n",
    "    # Extract from brand:, product:, cert:, http_path:, model:\n",
    "    for prefix in ['brand:', 'product:', 'cert:', 'http_path:', 'model:']:\n",
    "        if prefix in reason_str:\n",
    "            start = reason_str.index(prefix) + len(prefix)\n",
    "            end = reason_str.find(':', start)\n",
    "            if end == -1:\n",
    "                end = reason_str.find('(', start)\n",
    "            if end == -1:\n",
    "                end = len(reason_str)\n",
    "            brand = reason_str[start:end].strip()\n",
    "            return brand\n",
    "\n",
    "    if 'protocol:' in reason_str:\n",
    "        return 'Protocol Match'\n",
    "    if 'tag:' in reason_str:\n",
    "        return 'Tag Match'\n",
    "    if 'keyword' in reason_str:\n",
    "        return 'Keyword Match'\n",
    "\n",
    "    return 'Unknown'\n",
    "\n",
    "def analyze_brand_distribution(df_subset, device_type, reason_col):\n",
    "    \"\"\"Analyze brand distribution\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        return df_subset\n",
    "\n",
    "    df_subset['detected_brand'] = df_subset[reason_col].apply(extract_brand_from_reason)\n",
    "    brand_counts = df_subset['detected_brand'].value_counts()\n",
    "\n",
    "    print(f\"\\n{device_type} Brand Distribution:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Brand':<30} {'Count':>10} {'%':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for brand, count in brand_counts.head(15).items():\n",
    "        pct = (count / len(df_subset) * 100)\n",
    "        print(f\"{brand:<30} {count:>10,} {pct:>7.2f}%\")\n",
    "\n",
    "    if len(brand_counts) > 15:\n",
    "        other = brand_counts.iloc[15:].sum()\n",
    "        print(f\"{'Others':<30} {other:>10,} {other/len(df_subset)*100:>7.2f}%\")\n",
    "\n",
    "    return df_subset\n",
    "\n",
    "eacs_df = analyze_brand_distribution(eacs_df, 'EACS', 'eacs_reason')\n",
    "vss_df = analyze_brand_distribution(vss_df, 'VSS', 'vss_reason')\n",
    "ihas_df = analyze_brand_distribution(ihas_df, 'IHAS', 'ihas_reason')\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "170b3dcbfcb2fd24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CPSS outliers analyses",
   "id": "d0f0afedd8f85fa0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BAS Subcategory analysis",
   "id": "b6bc72a2dacc6269"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 11: BAS SUBCATEGORY ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "if eacs_df['is_bas'].sum() > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BAS SUBCATEGORY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    bas_df = eacs_df[eacs_df['is_bas']].copy()\n",
    "\n",
    "    print(f\"\\nBAS devices: {len(bas_df):,} ({len(bas_df)/len(eacs_df)*100:.1f}% of EACS)\")\n",
    "    print(f\"Pure EACS: {len(eacs_df) - len(bas_df):,} ({(len(eacs_df) - len(bas_df))/len(eacs_df)*100:.1f}% of EACS)\")\n",
    "\n",
    "    # BAS brands\n",
    "    if 'detected_brand' in bas_df.columns:\n",
    "        print(\"\\nBAS brands:\")\n",
    "        bas_brands = bas_df['detected_brand'].value_counts()\n",
    "        for brand, count in bas_brands.items():\n",
    "            print(f\"  {brand:<20} {count:>6,}\")\n",
    "\n",
    "    # BAS protocols\n",
    "    bas_protocols = bas_df['eacs_reason'].str.extract(r'protocol:(\\w+)', expand=False).value_counts()\n",
    "    if len(bas_protocols) > 0:\n",
    "        print(\"\\nBAS protocols:\")\n",
    "        for protocol, count in bas_protocols.items():\n",
    "            if pd.notna(protocol):\n",
    "                print(f\"  {protocol:<20} {count:>6,}\")\n",
    "\n",
    "    print(\"=\"*70)"
   ],
   "id": "64f485142640b2ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-function Device Analysis",
   "id": "feb03fe0bce9169a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 12: MULTI-FUNCTION DEVICE ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "if len(multi_function_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-FUNCTION DEVICE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nMulti-function devices: {len(multi_function_df):,}\")\n",
    "\n",
    "    # Category combinations\n",
    "    print(\"\\nCategory combinations:\")\n",
    "    for idx, row in multi_function_df.head(10).iterrows():\n",
    "        ip = row.get('ip', 'N/A')\n",
    "        categories = []\n",
    "        if row.get('is_eacs', False):\n",
    "            categories.append(f\"EACS({row.get('eacs_confidence', 0):.0f}%)\")\n",
    "        if row.get('is_vss', False):\n",
    "            categories.append(f\"VSS({row.get('vss_confidence', 0):.0f}%)\")\n",
    "        if row.get('is_ihas', False):\n",
    "            categories.append(f\"IHAS({row.get('ihas_confidence', 0):.0f}%)\")\n",
    "\n",
    "        brand = row.get('detected_brand', 'Unknown')\n",
    "        print(f\"  {ip:<20} {brand:<20} {' + '.join(categories)}\")\n",
    "\n",
    "    if len(multi_function_df) > 10:\n",
    "        print(f\"  ... and {len(multi_function_df) - 10} more\")\n",
    "\n",
    "    print(\"=\"*70)"
   ],
   "id": "d12c762a25e0feca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CSV Exports",
   "id": "ed5be7a1f4183b63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 9: CSV EXPORTS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING CSV FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define export columns (if they exist)\n",
    "export_columns_base = [\n",
    "    'ip', 'service.port', 'service.protocol',\n",
    "    'service.http.title', 'service.fingerprints.product', 'service.fingerprints.vendor',\n",
    "    'service.banner', 'asn.org', 'geo.country', 'geo.city',\n",
    "    'service.fingerprints.tags'\n",
    "]\n",
    "\n",
    "def export_cpss_category(df_subset, device_type, confidence_col, reason_col):\n",
    "    \"\"\"Export CPSS category to CSV with confidence scores\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        print(f\"\\n⚠️  No {device_type} devices to export\")\n",
    "        return\n",
    "\n",
    "    # Find available columns\n",
    "    export_cols = [col for col in export_columns_base if col in df_subset.columns]\n",
    "\n",
    "    # Add CPSS-specific columns\n",
    "    export_cols.extend([confidence_col, reason_col])\n",
    "\n",
    "    if 'detected_brand' in df_subset.columns:\n",
    "        export_cols.append('detected_brand')\n",
    "\n",
    "    # Add BAS flag for EACS\n",
    "    if device_type == 'EACS' and 'is_bas' in df_subset.columns:\n",
    "        export_cols.append('is_bas')\n",
    "\n",
    "    # Add multi-function flag\n",
    "    if 'is_multi_function' in df_subset.columns:\n",
    "        export_cols.append('is_multi_function')\n",
    "\n",
    "    # Export\n",
    "    filename = f'cpss_{device_type.lower()}_services_enhanced.csv'\n",
    "    filepath = output_dir / filename\n",
    "    df_subset[export_cols].to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"\\n {device_type} exported: {filename}\")\n",
    "    print(f\"  Services: {len(df_subset):,}\")\n",
    "    print(f\"  Avg confidence: {df_subset[confidence_col].mean():.1f}%\")\n",
    "    print(f\"  Columns: {len(export_cols)}\")\n",
    "\n",
    "    # High confidence sample\n",
    "    high_conf = df_subset[df_subset[confidence_col] >= 90]\n",
    "    if len(high_conf) > 0:\n",
    "        print(f\"\\n  High confidence sample (≥90%, first 3):\")\n",
    "        for idx, row in high_conf.head(3).iterrows():\n",
    "            ip = row.get('ip', 'N/A')\n",
    "            port = row.get('service.port', 'N/A')\n",
    "            brand = row.get('detected_brand', 'N/A')\n",
    "            conf = row.get(confidence_col, 0)\n",
    "            reason = str(row.get(reason_col, ''))[:40]\n",
    "            print(f\"    {ip}:{port} | {brand:<15} | {conf:>3.0f}% | {reason}\")\n",
    "\n",
    "# Export each category\n",
    "export_cpss_category(eacs_df, 'EACS', 'eacs_confidence', 'eacs_reason')\n",
    "export_cpss_category(vss_df, 'VSS', 'vss_confidence', 'vss_reason')\n",
    "export_cpss_category(ihas_df, 'IHAS', 'ihas_confidence', 'ihas_reason')\n",
    "\n",
    "# Export multi-function devices\n",
    "if len(multi_function_df) > 0:\n",
    "    multi_export_cols = [col for col in export_columns_base if col in multi_function_df.columns]\n",
    "    multi_export_cols.extend(['is_eacs', 'is_vss', 'is_ihas',\n",
    "                              'eacs_confidence', 'vss_confidence', 'ihas_confidence',\n",
    "                              'eacs_reason', 'vss_reason', 'ihas_reason'])\n",
    "\n",
    "    filepath = output_dir / 'cpss_multi_function_devices.csv'\n",
    "    multi_function_df[multi_export_cols].to_csv(filepath, index=False)\n",
    "    print(f\"\\n Multi-function devices exported: cpss_multi_function_devices.csv\")\n",
    "    print(f\"  Services: {len(multi_function_df):,}\")\n",
    "\n",
    "# Export combined CPSS dataset\n",
    "all_cpss = pd.concat([\n",
    "    eacs_df.assign(cpss_primary_category='EACS'),\n",
    "    vss_df.assign(cpss_primary_category='VSS'),\n",
    "    ihas_df.assign(cpss_primary_category='IHAS')\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(all_cpss) > 0:\n",
    "    export_cols = [col for col in export_columns_base if col in all_cpss.columns]\n",
    "    export_cols.extend(['cpss_primary_category',\n",
    "                       'eacs_confidence', 'vss_confidence', 'ihas_confidence',\n",
    "                       'eacs_reason', 'vss_reason', 'ihas_reason'])\n",
    "\n",
    "    if 'detected_brand' in all_cpss.columns:\n",
    "        export_cols.append('detected_brand')\n",
    "    if 'is_bas' in all_cpss.columns:\n",
    "        export_cols.append('is_bas')\n",
    "    if 'is_multi_function' in all_cpss.columns:\n",
    "        export_cols.append('is_multi_function')\n",
    "\n",
    "    filepath = output_dir / 'cpss_all_services_enhanced.csv'\n",
    "    all_cpss[[col for col in export_cols if col in all_cpss.columns]].to_csv(filepath, index=False)\n",
    "    print(f\"\\n Combined CPSS exported: cpss_all_services_enhanced.csv\")\n",
    "    print(f\"  Total services: {len(all_cpss):,}\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "7666c7d1d3e7e999",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisations",
   "id": "96828471f0da7edf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 10: VISUALIZATIONS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. CPSS Category Distribution (Pie)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "cpss_counts = pd.Series({\n",
    "    'EACS': len(eacs_df),\n",
    "    'VSS': len(vss_df),\n",
    "    'IHAS': len(ihas_df)\n",
    "})\n",
    "colors_pie = [COLORS['eacs'], COLORS['vss'], COLORS['ihas']]\n",
    "wedges, texts, autotexts = ax1.pie(cpss_counts.values, labels=cpss_counts.index,\n",
    "                                     autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(12)\n",
    "ax1.set_title('CPSS Category Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# 2. Services by Category (Bar)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars = ax2.bar(cpss_counts.index, cpss_counts.values, color=colors_pie,\n",
    "               edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Number of Services', fontsize=11)\n",
    "ax2.set_title('CPSS Services by Category', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for bar, count in zip(bars, cpss_counts.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, count,\n",
    "            f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 3. Confidence Score Distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "confidence_data = []\n",
    "confidence_labels = []\n",
    "if len(eacs_df) > 0:\n",
    "    confidence_data.append(eacs_df['eacs_confidence'].values)\n",
    "    confidence_labels.append('EACS')\n",
    "if len(vss_df) > 0:\n",
    "    confidence_data.append(vss_df['vss_confidence'].values)\n",
    "    confidence_labels.append('VSS')\n",
    "if len(ihas_df) > 0:\n",
    "    confidence_data.append(ihas_df['ihas_confidence'].values)\n",
    "    confidence_labels.append('IHAS')\n",
    "\n",
    "if confidence_data:\n",
    "    bp = ax3.boxplot(confidence_data, labels=confidence_labels, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors_pie[:len(confidence_data)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    ax3.set_ylabel('Confidence Score (%)', fontsize=11)\n",
    "    ax3.set_title('Confidence Score Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    ax3.set_ylim(0, 105)\n",
    "\n",
    "# 4. Detection Method Comparison\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "detection_methods = {\n",
    "    'Protocol': [],\n",
    "    'HTTP Path': [],\n",
    "    'Brand/Product': [],\n",
    "    'Tag': [],\n",
    "    'Keyword': []\n",
    "}\n",
    "\n",
    "for df_subset, reason_col, label in [(eacs_df, 'eacs_reason', 'EACS'),\n",
    "                                      (vss_df, 'vss_reason', 'VSS'),\n",
    "                                      (ihas_df, 'ihas_reason', 'IHAS')]:\n",
    "    if len(df_subset) > 0:\n",
    "        total = len(df_subset)\n",
    "        detection_methods['Protocol'].append(len(df_subset[df_subset[reason_col].str.contains('protocol:', na=False)]) / total * 100)\n",
    "        detection_methods['HTTP Path'].append(len(df_subset[df_subset[reason_col].str.contains('http_path:', na=False)]) / total * 100)\n",
    "        detection_methods['Brand/Product'].append(len(df_subset[df_subset[reason_col].str.contains('brand:|product:|model:', na=False)]) / total * 100)\n",
    "        detection_methods['Tag'].append(len(df_subset[df_subset[reason_col].str.contains('tag:', na=False)]) / total * 100)\n",
    "        detection_methods['Keyword'].append(len(df_subset[df_subset[reason_col].str.contains('keyword', na=False)]) / total * 100)\n",
    "    else:\n",
    "        for key in detection_methods:\n",
    "            detection_methods[key].append(0)\n",
    "\n",
    "x = np.arange(len(['EACS', 'VSS', 'IHAS']))\n",
    "width = 0.15\n",
    "multiplier = 0\n",
    "\n",
    "for method, values in detection_methods.items():\n",
    "    offset = width * multiplier\n",
    "    bars = ax4.bar(x + offset, values, width, label=method)\n",
    "    multiplier += 1\n",
    "\n",
    "ax4.set_ylabel('Percentage (%)', fontsize=11)\n",
    "ax4.set_title('Detection Method Comparison by Category', fontsize=14, fontweight='bold', pad=15)\n",
    "ax4.set_xticks(x + width * 2)\n",
    "ax4.set_xticklabels(['EACS', 'VSS', 'IHAS'])\n",
    "ax4.legend(loc='upper right', fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Top Brands (Combined)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "all_brands = []\n",
    "if len(eacs_df) > 0 and 'detected_brand' in eacs_df.columns:\n",
    "    all_brands.extend(eacs_df['detected_brand'].tolist())\n",
    "if len(vss_df) > 0 and 'detected_brand' in vss_df.columns:\n",
    "    all_brands.extend(vss_df['detected_brand'].tolist())\n",
    "if len(ihas_df) > 0 and 'detected_brand' in ihas_df.columns:\n",
    "    all_brands.extend(ihas_df['detected_brand'].tolist())\n",
    "\n",
    "if all_brands:\n",
    "    brand_counts = pd.Series(all_brands).value_counts().head(15)\n",
    "    ax5.barh(range(len(brand_counts)), brand_counts.values, color=COLORS['quaternary'])\n",
    "    ax5.set_yticks(range(len(brand_counts)))\n",
    "    ax5.set_yticklabels(brand_counts.index)\n",
    "    ax5.set_xlabel('Number of Services', fontsize=11)\n",
    "    ax5.set_title('Top 15 CPSS Brands (All Categories)', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    for i, (brand, count) in enumerate(brand_counts.items()):\n",
    "        ax5.text(count, i, f' {count:,}', va='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Enhanced CPSS Identification Summary', fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.savefig(output_dir / 'cpss_identification_enhanced_summary.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization saved: cpss_identification_enhanced_summary.jpg\")\n",
    "print(\"=\"*70)"
   ],
   "id": "4a85816f59bf0028",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summary Report",
   "id": "7176aee32a6020f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 13: SUMMARY REPORT\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "ENHANCED CPSS IDENTIFICATION SUMMARY REPORT\n",
    "{'='*70}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "DATASET OVERVIEW\n",
    "{'-'*70}\n",
    "Total services analyzed:          {len(df):>12,}\n",
    "\n",
    "CPSS IDENTIFICATION RESULTS\n",
    "{'-'*70}\n",
    "EACS (Access Control):            {len(eacs_df):>12,} ({len(eacs_df)/len(df)*100:>6.2f}%)\n",
    "  └─ BAS subcategory:             {eacs_df['is_bas'].sum():>12,} ({eacs_df['is_bas'].sum()/len(eacs_df)*100 if len(eacs_df)>0 else 0:>6.2f}% of EACS)\n",
    "  └─ Average confidence:          {eacs_df['eacs_confidence'].mean() if len(eacs_df)>0 else 0:>12.1f}%\n",
    "\n",
    "VSS (Video Surveillance):         {len(vss_df):>12,} ({len(vss_df)/len(df)*100:>6.2f}%)\n",
    "  └─ Average confidence:          {vss_df['vss_confidence'].mean() if len(vss_df)>0 else 0:>12.1f}%\n",
    "\n",
    "IHAS (Intrusion & Alarm):        {len(ihas_df):>12,} ({len(ihas_df)/len(df)*100:>6.2f}%)\n",
    "  └─ Average confidence:          {ihas_df['ihas_confidence'].mean() if len(ihas_df)>0 else 0:>12.1f}%\n",
    "\n",
    "Total CPSS devices:               {total_cpss:>12,} ({total_cpss/len(df)*100:>6.2f}%)\n",
    "Multi-function devices:           {len(multi_function_df):>12,}\n",
    "Non-CPSS services:                {len(df)-total_cpss:>12,} ({(len(df)-total_cpss)/len(df)*100:>6.2f}%)\n",
    "\n",
    "ENHANCEMENT FEATURES APPLIED\n",
    "{'-'*70}\n",
    " Protocol detection (RTSP, ONVIF, BACnet, SIA DC-09, Contact ID)\n",
    " HTTP path pattern matching\n",
    " Model number detection\n",
    " Confidence scoring system (0-100%)\n",
    " Multi-function device handling\n",
    " BAS subcategory flagging\n",
    " Enhanced cloud provider exclusions\n",
    "\n",
    "TOP DETECTION METHODS\n",
    "{'-'*70}\n",
    "EACS:  {eacs_df['eacs_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(eacs_df)>0 else 'N/A':<30} {eacs_df['eacs_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(eacs_df)>0 else 0:>6,}\n",
    "VSS:   {vss_df['vss_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(vss_df)>0 else 'N/A':<30} {vss_df['vss_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(vss_df)>0 else 0:>6,}\n",
    "IHAS: {ihas_df['ihas_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(ihas_df)>0 else 'N/A':<30} {ihas_df['ihas_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(ihas_df)>0 else 0:>6,}\n",
    "\n",
    "FILES GENERATED\n",
    "{'-'*70}\n",
    "cpss_eacs_services_enhanced.csv   {len(eacs_df):>12,} services\n",
    "cpss_vss_services_enhanced.csv    {len(vss_df):>12,} services\n",
    "cpss_ihas_services_enhanced.csv   {len(ihas_df):>12,} services\n",
    "cpss_all_services_enhanced.csv    {total_cpss:>12,} services\n",
    "cpss_multi_function_devices.csv   {len(multi_function_df):>12,} services\n",
    "cpss_identification_enhanced_summary.jpg\n",
    "\n",
    "NEXT STEPS\n",
    "{'-'*70}\n",
    "1. Validate CSV exports\n",
    "2. Review high-confidence matches (≥90%)\n",
    "3. Investigate multi-function devices\n",
    "4. Proceed to ISO controls assessment (analyses_3)\n",
    "\n",
    "{'='*70}\n",
    "End of Report\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save report\n",
    "report_path = output_dir / 'cpss_identification_enhanced_report.txt'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\n Summary report saved: cpss_identification_enhanced_report.txt\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENHANCED CPSS IDENTIFICATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n All files saved to: {output_dir.absolute()}\")\n",
    "print(\"\\nEnhancement Summary:\")\n",
    "print(f\"  • {len(eacs_df):,} EACS devices (avg {eacs_df['eacs_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {len(vss_df):,} VSS devices (avg {vss_df['vss_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {len(ihas_df):,} IHAS devices (avg {ihas_df['ihas_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {total_cpss:,} total CPSS devices identified\")\n",
    "print(\"\\nReview the CSV files and proceed to ISO control assessment!\")\n",
    "print(\"=\"*70)"
   ],
   "id": "4411768948df8701",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
