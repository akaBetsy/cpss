{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# CPSS IDENTIFICATION NOTEBOOK\n",
    "Integrates enhanced detection for:\n",
    "- EACS (Electronic Access Control Systems) + BAS\n",
    "- VSS (Video Surveillance Systems)\n",
    "- I&HAS (Intrusion & Hold-up Alarm Systems)\n",
    "\n",
    "Features:\n",
    "- 40 brands total (100% thesis Appendix C coverage)\n",
    "- Protocol detection (RTSP, ONVIF, BACnet, SIA DC-09, etc.)\n",
    "- HTTP path matching (70+ paths)\n",
    "- Model number detection (100+ patterns)\n",
    "- Confidence scoring (0-100%)\n",
    "- Multi-function device handling\n",
    "- BAS subcategory flagging\n",
    "- Enhanced cloud/IT exclusions\n",
    "\n",
    "**Brand Coverage:**\n",
    "- **VSS**: 15 brands (Hikvision, Dahua, Axis, MOBOTIX, Geutebruck, etc.)\n",
    "- **EACS**: 13 brands (Nedap, Paxton, Genetec, + 4 BAS brands)\n",
    "- **I&HAS**: 12 brands (AJAX, Vanderbilt, Honeywell, Bosch, etc.)\n",
    "- **Total**: 40 brands with 100% thesis Appendix C coverage\n",
    "\n",
    "**Device Categories:**\n",
    "1. **EACS** - Electronic Access Control Systems\n",
    "   - Includes BAS (Building Automation Systems) as subcategory\n",
    "   - Multi-function flag for devices like Genetec\n",
    "2. **VSS** - Video Surveillance Systems\n",
    "   - IP cameras, NVRs, DVRs, VMS platforms\n",
    "3. **I&HAS** - Intrusion & Hold-up Alarm Systems\n",
    "   - Alarm panels, intrusion detection, sensors\n",
    "\n",
    "**Expected Runtime:** ~15 minutes\n",
    "**Expected Precision:** 85-95%"
   ],
   "id": "9cb1a54e5b988254"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration and Setup",
   "id": "f408f56db60c3845"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import & Setup",
   "id": "668e354288809920"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:30.677013100Z",
     "start_time": "2026-01-05T13:50:30.639084300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 2: IMPORTS & SETUP\n",
    "# ========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path('./output/4_validation/2_cpss_identification/shodan')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Color palette\n",
    "COLORS = {\n",
    "    'eacs': '#7C3AED',      # Purple\n",
    "    'vss': '#14B8A6',       # Turquoise\n",
    "    'ihas': '#F59E0B',      # Yellow\n",
    "    'quaternary': '#22C55E', # Emerald\n",
    "    'danger': '#EF4444',\n",
    "    'neutral': '#9CA3AF',\n",
    "}\n",
    "\n",
    "print(f\"Enhanced CPSS Identification started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output directory: {output_dir.absolute()}\")"
   ],
   "id": "8af817bdd9c61703",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced CPSS Identification started: 2026-01-05 14:50:30\n",
      "Output directory: H:\\_HHS_thesis\\GitHub\\thesis\\v1\\output\\4_validation\\2_cpss_identification\\shodan\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data",
   "id": "eec3a28ce89f0ac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:32.748820300Z",
     "start_time": "2026-01-05T13:50:30.681014100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 3: LOAD DATA\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv('./staging/4_validation/5_shodan_20251109.csv', low_memory=False)\n",
    "print(f\"Loaded {len(df):,} total services\")\n",
    "print(f\"Dataset has {len(df.columns)} columns\")\n",
    "print(\"=\"*70)\n"
   ],
   "id": "54eb73cb51ceba90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "Loaded 10,000 total services\n",
      "Dataset has 368 columns\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEV (Known Exploited Vulnerabilities) Calculation\n",
    "\n",
    "The `is_kev` flag indicates whether a service has CVEs listed in CISA's KEV catalog.\n",
    "This is critical for the A.8.8 ISO control assessment and risk scoring."
   ],
   "id": "kev_explanation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:32.849889100Z",
     "start_time": "2026-01-05T13:50:32.835703700Z"
    }
   },
   "source": [
    "\n",
    "# ========================================================================\n",
    "# KEV (Known Exploited Vulnerabilities) CALCULATION\n",
    "# ========================================================================\n",
    "def calculate_is_kev(df):\n",
    "    \"\"\"\n",
    "    Calculate is_kev flag based on service.cves column\n",
    "    \n",
    "    This checks if any CVE in the service.cves field is listed in\n",
    "    CISA's Known Exploited Vulnerabilities catalog.\n",
    "    \n",
    "    Note: KEV data should already be embedded in the CVE data by MODAT.\n",
    "    If service.cves contains 'KEV' marker or specific KEV CVEs, this\n",
    "    will be detected.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    def check_kev(cves_field):\n",
    "        \"\"\"Check if CVE field indicates KEV presence\"\"\"\n",
    "        if pd.isna(cves_field) or str(cves_field).strip() == '':\n",
    "            return False\n",
    "        \n",
    "        cves_str = str(cves_field).upper()\n",
    "        \n",
    "        # Method 1: Check for explicit KEV marker (if MODAT adds it)\n",
    "        if 'KEV' in cves_str:\n",
    "            return True\n",
    "        \n",
    "        # Method 2: Check for known KEV CVEs (would need KEV list)\n",
    "        # For now, we'll assume MODAT data already marks KEVs\n",
    "        # If not, this function should be enhanced with actual KEV list\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    df['is_kev'] = df['service.cves'].apply(check_kev)\n",
    "    \n",
    "    kev_count = df['is_kev'].sum()\n",
    "    print(f\"  KEV calculation: {kev_count:,} services with Known Exploited Vulnerabilities\")\n",
    "    \n",
    "    return df\n",
    ""
   ],
   "id": "kev_calculation_function",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Enhanced Detection Modules",
   "id": "2b2514dc7bfe7ff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:32.901734300Z",
     "start_time": "2026-01-05T13:50:32.852888900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 4: LOAD ENHANCED DETECTION MODULES (BETTER VERSION)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING ENHANCED DETECTION MODULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Function to load modules properly\n",
    "def load_detection_module(module_name, file_path):\n",
    "    \"\"\"Load a Python module and return it\"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    if spec is None:\n",
    "        raise ImportError(f\"Could not load {module_name} from {file_path}\")\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# Load the three detection modules\n",
    "print(\"\\nLoading EACS enhanced detection...\")\n",
    "eacs_mod = load_detection_module('eacs_enhanced', '8a_EACS_enhanced_detection.py')\n",
    "\n",
    "print(\"\\nLoading VSS enhanced detection...\")\n",
    "vss_mod = load_detection_module('vss_enhanced', '8b_VSS_enhanced_detection.py')\n",
    "\n",
    "print(\"\\nLoading I&HAS enhanced detection...\")\n",
    "ihas_mod = load_detection_module('ihas_enhanced', '8c_IHAS_enhanced_detection.py')\n",
    "\n",
    "\n",
    "# Extract the functions and configs (NOW IDE WILL RECOGNIZE THEM)\n",
    "identify_eacs_enhanced = eacs_mod.identify_eacs_enhanced\n",
    "EACS_ENHANCED_CONFIG = eacs_mod.EACS_ENHANCED_CONFIG\n",
    "\n",
    "identify_ihas_enhanced = ihas_mod.identify_ihas_enhanced\n",
    "IHAS_ENHANCED_CONFIG = ihas_mod.IHAS_ENHANCED_CONFIG\n",
    "\n",
    "identify_vss_enhanced = vss_mod.identify_vss_enhanced\n",
    "VSS_ENHANCED_CONFIG = vss_mod.VSS_ENHANCED_CONFIG\n",
    "\n",
    "print(\"\\nAll enhanced detection modules loaded\")\n",
    "# print(f\"  EACS: {len(EACS_ENHANCED_CONFIG['brands'])} brands, {len(EACS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "# print(f\"  I&HAS: {len(IHAS_ENHANCED_CONFIG['brands'])} brands, {len(IHAS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "# print(f\"  VSS: {len(VSS_ENHANCED_CONFIG['brands'])} brands, {len(VSS_ENHANCED_CONFIG['protocols'])} protocols\")\n",
    "print(\"=\"*70)"
   ],
   "id": "329409013885b545",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING ENHANCED DETECTION MODULES\n",
      "======================================================================\n",
      "\n",
      "Loading EACS enhanced detection...\n",
      "Comprehensive EACS detection loaded\n",
      "\n",
      "\n",
      "Loading VSS enhanced detection...\n",
      "Comprehensive VSS detection loaded\n",
      "\n",
      "\n",
      "Loading I&HAS enhanced detection...\n",
      "Comprehensive I&HAS detection loaded\n",
      "\n",
      "\n",
      "All enhanced detection modules loaded\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Identification",
   "id": "bc004ba35bad3bf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:32.943895600Z",
     "start_time": "2026-01-05T13:50:32.906609100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# COLUMN NAME MAPPING\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COLUMN NAME MAPPING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rename_mapping = {\n",
    "    # HTTP fields\n",
    "    'service.http.title': 'http.html_title',\n",
    "    'service.http.path': 'http.path',\n",
    "    'service.http.headers': 'http.headers',\n",
    "\n",
    "    # TLS fields\n",
    "    'service.tls.issuer.common_name': 'ssl.cert.issuer',\n",
    "    'service.tls.subject.common_name': 'ssl.cert.subject',\n",
    "}\n",
    "\n",
    "print(\"\\nApplying column name mapping...\")\n",
    "for old_name, new_name in rename_mapping.items():\n",
    "    if old_name in df.columns and new_name not in df.columns:\n",
    "        df.rename(columns={old_name: new_name}, inplace=True)\n",
    "        print(f\"Renamed: {old_name} → {new_name}\")\n",
    "    elif new_name in df.columns:\n",
    "        print(f\"{new_name} already exists\")\n",
    "    else:\n",
    "        print(f\"{old_name} not found in dataset\")\n",
    "\n",
    "# Verify key columns exist\n",
    "print(\"\\nVerifying columns needed by detection scripts:\")\n",
    "required_columns = [\n",
    "    'service.fingerprints.tags',\n",
    "    'http.html_title',      # or service.http.title\n",
    "    # 'http.path',            # or service.http.path\n",
    "    'http.headers',         # or service.http.headers\n",
    "    'service.http.body',    # NEW - should exist as-is\n",
    "    'service.banner',\n",
    "    'service.fingerprints.os.product',\n",
    "    'service.fingerprints.service.product',\n",
    "    'service.port',\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col in df.columns:\n",
    "        non_null = df[col].notna().sum()\n",
    "        print(f\"  ✓ {col:<30} ({non_null:,} non-null)\")\n",
    "    else:\n",
    "        print(f\"  ✗ {col:<30} MISSING!\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "41defe25777566e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COLUMN NAME MAPPING\n",
      "======================================================================\n",
      "\n",
      "Applying column name mapping...\n",
      "Renamed: service.http.title → http.html_title\n",
      "service.http.path not found in dataset\n",
      "service.http.headers not found in dataset\n",
      "service.tls.issuer.common_name not found in dataset\n",
      "service.tls.subject.common_name not found in dataset\n",
      "\n",
      "Verifying columns needed by detection scripts:\n",
      "  ✗ service.fingerprints.tags      MISSING!\n",
      "  ✓ http.html_title                (1,166 non-null)\n",
      "  ✗ http.headers                   MISSING!\n",
      "  ✗ service.http.body              MISSING!\n",
      "  ✗ service.banner                 MISSING!\n",
      "  ✗ service.fingerprints.os.product MISSING!\n",
      "  ✗ service.fingerprints.service.product MISSING!\n",
      "  ✓ service.port                   (10,000 non-null)\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:48.380551100Z",
     "start_time": "2026-01-05T13:50:32.945895400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 5: RUN ENHANCED IDENTIFICATION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENHANCED IDENTIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================\n",
    "# 1. IDENTIFY EACS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n1. Identifying EACS devices...\")\n",
    "eacs_results = df.apply(lambda row: identify_eacs_enhanced(row), axis=1)\n",
    "\n",
    "# Extract from dictionary (not tuple!)\n",
    "df['is_eacs'] = eacs_results.apply(lambda x: x['is_eacs'])\n",
    "df['eacs_confidence'] = eacs_results.apply(lambda x: x['eacs_confidence'])\n",
    "df['eacs_reason'] = eacs_results.apply(lambda x: x['eacs_reason'])\n",
    "df['detected_eacs_brand'] = eacs_results.apply(lambda x: x['detected_brand'])\n",
    "df['detected_eacs_product'] = eacs_results.apply(lambda x: x['detected_product'])\n",
    "df['is_bas'] = eacs_results.apply(lambda x: x['is_bas'])\n",
    "\n",
    "# NEW: Extract detailed match info\n",
    "df['eacs_match_field'] = eacs_results.apply(lambda x: x.get('match_field'))\n",
    "df['eacs_match_pattern'] = eacs_results.apply(lambda x: x.get('match_pattern'))\n",
    "df['eacs_match_value'] = eacs_results.apply(lambda x: x.get('match_value'))\n",
    "\n",
    "eacs_count = df['is_eacs'].sum()\n",
    "bas_count = df['is_bas'].sum()\n",
    "print(f\"   Found {eacs_count:,} EACS services\")\n",
    "print(f\"   Found {bas_count:,} BAS services (subset of EACS)\")\n",
    "if eacs_count > 0:\n",
    "    print(f\"   Average confidence: {df[df['is_eacs']]['eacs_confidence'].mean():.1f}%\")\n",
    "\n",
    "# ========================================\n",
    "# 2. IDENTIFY VSS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n2. Identifying VSS devices...\")\n",
    "vss_results = df.apply(lambda row: identify_vss_enhanced(row), axis=1)\n",
    "\n",
    "# Extract from dictionary\n",
    "df['is_vss'] = vss_results.apply(lambda x: x['is_vss'])\n",
    "df['vss_confidence'] = vss_results.apply(lambda x: x['vss_confidence'])\n",
    "df['vss_reason'] = vss_results.apply(lambda x: x['vss_reason'])\n",
    "df['detected_vss_brand'] = vss_results.apply(lambda x: x['detected_brand'])\n",
    "df['detected_vss_product'] = vss_results.apply(lambda x: x['detected_product'])\n",
    "\n",
    "# NEW: Extract detailed match info\n",
    "df['vss_match_field'] = vss_results.apply(lambda x: x.get('match_field'))\n",
    "df['vss_match_pattern'] = vss_results.apply(lambda x: x.get('match_pattern'))\n",
    "df['vss_match_value'] = vss_results.apply(lambda x: x.get('match_value'))\n",
    "\n",
    "vss_count = df['is_vss'].sum()\n",
    "print(f\"   Found {vss_count:,} VSS services\")\n",
    "if vss_count > 0:\n",
    "    print(f\"   Average confidence: {df[df['is_vss']]['vss_confidence'].mean():.1f}%\")\n",
    "\n",
    "# ========================================\n",
    "# 3. IDENTIFY I&HAS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n3. Identifying I&HAS devices...\")\n",
    "ihas_results = df.apply(lambda row: identify_ihas_enhanced(row), axis=1)\n",
    "\n",
    "# Extract from dictionary\n",
    "df['is_ihas'] = ihas_results.apply(lambda x: x['is_ihas'])\n",
    "df['ihas_confidence'] = ihas_results.apply(lambda x: x['ihas_confidence'])\n",
    "df['ihas_reason'] = ihas_results.apply(lambda x: x['ihas_reason'])\n",
    "df['detected_ihas_brand'] = ihas_results.apply(lambda x: x['detected_brand'])\n",
    "df['detected_ihas_product'] = ihas_results.apply(lambda x: x['detected_product'])\n",
    "\n",
    "# NEW: Extract detailed match info\n",
    "df['ihas_match_field'] = ihas_results.apply(lambda x: x.get('match_field'))\n",
    "df['ihas_match_pattern'] = ihas_results.apply(lambda x: x.get('match_pattern'))\n",
    "df['ihas_match_value'] = ihas_results.apply(lambda x: x.get('match_value'))\n",
    "\n",
    "ihas_count = df['is_ihas'].sum()\n",
    "print(f\"   Found {ihas_count:,} I&HAS services\")\n",
    "if ihas_count > 0:\n",
    "    print(f\"   Average confidence: {df[df['is_ihas']]['ihas_confidence'].mean():.1f}%\")\n",
    "\n",
    "# ========================================\n",
    "# 4. MULTI-FUNCTION DETECTION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n4. Identifying multi-function devices...\")\n",
    "df['cpss_category_count'] = df['is_eacs'].astype(int) + df['is_vss'].astype(int) + df['is_ihas'].astype(int)\n",
    "df['is_cpss'] = df['cpss_category_count'] > 0\n",
    "df['is_multi_function'] = df['cpss_category_count'] > 1\n",
    "\n",
    "multi_count = df['is_multi_function'].sum()\n",
    "print(f\"   Found {multi_count:,} multi-function devices\")\n",
    "\n",
    "# ========================================\n",
    "# 5. SUMMARY STATISTICS\n",
    "# ========================================\n",
    "\n",
    "total_cpss = df['is_cpss'].sum()\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IDENTIFICATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total services:          {len(df):>12,}\")\n",
    "print(f\"EACS services:           {eacs_count:>12,} ({eacs_count/len(df)*100:>6.2f}%)\")\n",
    "print(f\"  └─ BAS subcategory:    {bas_count:>12,} ({bas_count/eacs_count*100 if eacs_count>0 else 0:>6.2f}% of EACS)\")\n",
    "print(f\"VSS services:            {vss_count:>12,} ({vss_count/len(df)*100:>6.2f}%)\")\n",
    "print(f\"I&HAS services:          {ihas_count:>12,} ({ihas_count/len(df)*100:>6.2f}%)\")\n",
    "print(f\"Multi-function devices:  {multi_count:>12,}\")\n",
    "print(f\"Total CPSS devices:      {total_cpss:>12,} ({total_cpss/len(df)*100:>6.2f}%)\")\n",
    "print(f\"Non-CPSS services:       {len(df)-total_cpss:>12,} ({(len(df)-total_cpss)/len(df)*100:>6.2f}%)\")\n",
    "print(\"=\"*70)"
   ],
   "id": "dd56d69a6bf78ff2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ENHANCED IDENTIFICATION\n",
      "======================================================================\n",
      "\n",
      "1. Identifying EACS devices...\n",
      "   Found 0 EACS services\n",
      "   Found 0 BAS services (subset of EACS)\n",
      "\n",
      "2. Identifying VSS devices...\n",
      "   Found 0 VSS services\n",
      "\n",
      "3. Identifying I&HAS devices...\n",
      "   Found 0 I&HAS services\n",
      "\n",
      "4. Identifying multi-function devices...\n",
      "   Found 0 multi-function devices\n",
      "\n",
      "======================================================================\n",
      "IDENTIFICATION SUMMARY\n",
      "======================================================================\n",
      "Total services:                10,000\n",
      "EACS services:                      0 (  0.00%)\n",
      "  └─ BAS subcategory:               0 (  0.00% of EACS)\n",
      "VSS services:                       0 (  0.00%)\n",
      "I&HAS services:                     0 (  0.00%)\n",
      "Multi-function devices:             0\n",
      "Total CPSS devices:                 0 (  0.00%)\n",
      "Non-CPSS services:             10,000 (100.00%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Confidence Analysis",
   "id": "b317725fa1ba45c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:48.434989200Z",
     "start_time": "2026-01-05T13:50:48.397917300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CREATE FILTERED DATAFRAMES\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING FILTERED DATAFRAMES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Filter by category\n",
    "eacs_df = df[df['is_eacs']].copy()\n",
    "vss_df = df[df['is_vss']].copy()\n",
    "ihas_df = df[df['is_ihas']].copy()\n",
    "multi_function_df = df[df['is_multi_function']].copy()\n",
    "\n",
    "print(f\"\\nFiltered DataFrames created:\")\n",
    "print(f\"  EACS:            {len(eacs_df):>8,} rows\")\n",
    "print(f\"  VSS:             {len(vss_df):>8,} rows\")\n",
    "print(f\"  I&HAS:           {len(ihas_df):>8,} rows\")\n",
    "print(f\"  Multi-function:  {len(multi_function_df):>8,} rows\")\n",
    "\n",
    "# ========================================\n",
    "# CONFIDENCE ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "def analyze_confidence(filtered_df, category_name, confidence_col):\n",
    "    \"\"\"Analyze confidence score distribution\"\"\"\n",
    "    if len(filtered_df) == 0:\n",
    "        print(f\"\\n{category_name}: No devices detected\")\n",
    "        return\n",
    "\n",
    "    total = len(filtered_df)\n",
    "    high = len(filtered_df[filtered_df[confidence_col] >= 90])\n",
    "    medium_high = len(filtered_df[(filtered_df[confidence_col] >= 80) & (filtered_df[confidence_col] < 90)])\n",
    "    medium = len(filtered_df[(filtered_df[confidence_col] >= 70) & (filtered_df[confidence_col] < 80)])\n",
    "    low = len(filtered_df[filtered_df[confidence_col] < 70])\n",
    "\n",
    "    print(f\"\\n{category_name} Confidence Distribution:\")\n",
    "    print(f\"  Total devices:     {total:>6,}\")\n",
    "    print(f\"    High (90-100%):    {high:>6,} ({high/total*100:>5.1f}%)\")\n",
    "    print(f\"    Medium (80-89%):   {medium_high:>6,} ({medium_high/total*100:>5.1f}%)\")\n",
    "    print(f\"    Medium (70-79%):   {medium:>6,} ({medium/total*100:>5.1f}%)\")\n",
    "    print(f\"    Low (<70%):        {low:>6,} ({low/total*100:>5.1f}%)\")\n",
    "\n",
    "analyze_confidence(eacs_df, 'EACS', 'eacs_confidence')\n",
    "analyze_confidence(vss_df, 'VSS', 'vss_confidence')\n",
    "analyze_confidence(ihas_df, 'IHAS', 'ihas_confidence')"
   ],
   "id": "b98cd74b43e6a9f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING FILTERED DATAFRAMES\n",
      "======================================================================\n",
      "\n",
      "Filtered DataFrames created:\n",
      "  EACS:                   0 rows\n",
      "  VSS:                    0 rows\n",
      "  I&HAS:                  0 rows\n",
      "  Multi-function:         0 rows\n",
      "\n",
      "EACS: No devices detected\n",
      "\n",
      "VSS: No devices detected\n",
      "\n",
      "IHAS: No devices detected\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detection Method",
   "id": "6436db2c1e9de896"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:48.474809500Z",
     "start_time": "2026-01-05T13:50:48.440347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 7: DETECTION METHOD BREAKDOWN\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETECTION METHOD ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def analyze_detection_methods(df_subset, device_type, reason_col):\n",
    "    \"\"\"Analyze how devices were detected\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{device_type} Detection Methods:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Categorize detection methods\n",
    "    protocol_count = len(df_subset[df_subset[reason_col].str.contains('protocol:', na=False)])\n",
    "    tag_count = len(df_subset[df_subset[reason_col].str.contains('tag:', na=False)])\n",
    "    http_path_count = len(df_subset[df_subset[reason_col].str.contains('http_path:', na=False)])\n",
    "    brand_count = len(df_subset[df_subset[reason_col].str.contains('brand:', na=False)])\n",
    "    product_count = len(df_subset[df_subset[reason_col].str.contains('product:', na=False)])\n",
    "    model_count = len(df_subset[df_subset[reason_col].str.contains('model:', na=False)])\n",
    "    cert_count = len(df_subset[df_subset[reason_col].str.contains('cert:', na=False)])\n",
    "    keyword_count = len(df_subset[df_subset[reason_col].str.contains('keyword', na=False)])\n",
    "\n",
    "    total = len(df_subset)\n",
    "\n",
    "    print(f\"  Protocol detection:    {protocol_count:>6,} ({protocol_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Tag match:             {tag_count:>6,} ({tag_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  HTTP path match:       {http_path_count:>6,} ({http_path_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Brand match:           {brand_count:>6,} ({brand_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Product match:         {product_count:>6,} ({product_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Model number match:    {model_count:>6,} ({model_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Certificate match:     {cert_count:>6,} ({cert_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Keyword match:         {keyword_count:>6,} ({keyword_count/total*100:>5.1f}%)\")\n",
    "\n",
    "    # Top specific reasons\n",
    "    print(f\"\\n  Top 10 specific detection reasons:\")\n",
    "    reasons = df_subset[reason_col].value_counts().head(10)\n",
    "    for reason, count in reasons.items():\n",
    "        if not str(reason).startswith('excluded'):\n",
    "            print(f\"    {str(reason)[:55]:<55} {count:>6,}\")\n",
    "\n",
    "analyze_detection_methods(eacs_df, 'EACS', 'eacs_reason')\n",
    "analyze_detection_methods(vss_df, 'VSS', 'vss_reason')\n",
    "analyze_detection_methods(ihas_df, 'IHAS', 'ihas_reason')\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "3fb4fe056729456f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DETECTION METHOD ANALYSIS\n",
      "======================================================================\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Brand Distribution",
   "id": "e9b2e27fd790ce6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:48.506066300Z",
     "start_time": "2026-01-05T13:50:48.475812500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 8: BRAND DISTRIBUTION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BRAND DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def extract_brand_from_reason(reason):\n",
    "    \"\"\"Extract brand name from match reason\"\"\"\n",
    "    if pd.isna(reason):\n",
    "        return 'Unknown'\n",
    "\n",
    "    reason_str = str(reason)\n",
    "\n",
    "    # Extract from brand:, product:, cert:, http_path:, model:\n",
    "    for prefix in ['brand:', 'product:', 'cert:', 'http_path:', 'model:']:\n",
    "        if prefix in reason_str:\n",
    "            start = reason_str.index(prefix) + len(prefix)\n",
    "            end = reason_str.find(':', start)\n",
    "            if end == -1:\n",
    "                end = reason_str.find('(', start)\n",
    "            if end == -1:\n",
    "                end = len(reason_str)\n",
    "            brand = reason_str[start:end].strip()\n",
    "            return brand\n",
    "\n",
    "    if 'protocol:' in reason_str:\n",
    "        return 'Protocol Match'\n",
    "    if 'tag:' in reason_str:\n",
    "        return 'Tag Match'\n",
    "    if 'keyword' in reason_str:\n",
    "        return 'Keyword Match'\n",
    "\n",
    "    return 'Unknown'\n",
    "\n",
    "def analyze_brand_distribution(df_subset, device_type, reason_col):\n",
    "    \"\"\"Analyze brand distribution\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        return df_subset\n",
    "\n",
    "    df_subset['detected_brand'] = df_subset[reason_col].apply(extract_brand_from_reason)\n",
    "    brand_counts = df_subset['detected_brand'].value_counts()\n",
    "\n",
    "    print(f\"\\n{device_type} Brand Distribution:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Brand':<30} {'Count':>10} {'%':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for brand, count in brand_counts.head(15).items():\n",
    "        pct = (count / len(df_subset) * 100)\n",
    "        print(f\"{brand:<30} {count:>10,} {pct:>7.2f}%\")\n",
    "\n",
    "    if len(brand_counts) > 15:\n",
    "        other = brand_counts.iloc[15:].sum()\n",
    "        print(f\"{'Others':<30} {other:>10,} {other/len(df_subset)*100:>7.2f}%\")\n",
    "\n",
    "    return df_subset\n",
    "\n",
    "eacs_df = analyze_brand_distribution(eacs_df, 'EACS', 'eacs_reason')\n",
    "vss_df = analyze_brand_distribution(vss_df, 'VSS', 'vss_reason')\n",
    "ihas_df = analyze_brand_distribution(ihas_df, 'IHAS', 'ihas_reason')\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "170b3dcbfcb2fd24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BRAND DISTRIBUTION ANALYSIS\n",
      "======================================================================\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CPSS outliers analyses",
   "id": "d0f0afedd8f85fa0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BAS Subcategory analysis",
   "id": "b6bc72a2dacc6269"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:48.521762500Z",
     "start_time": "2026-01-05T13:50:48.510537500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 11: BAS SUBCATEGORY ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "if eacs_df['is_bas'].sum() > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BAS SUBCATEGORY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    bas_df = eacs_df[eacs_df['is_bas']].copy()\n",
    "\n",
    "    print(f\"\\nBAS devices: {len(bas_df):,} ({len(bas_df)/len(eacs_df)*100:.1f}% of EACS)\")\n",
    "    print(f\"Pure EACS: {len(eacs_df) - len(bas_df):,} ({(len(eacs_df) - len(bas_df))/len(eacs_df)*100:.1f}% of EACS)\")\n",
    "\n",
    "    # BAS brands\n",
    "    if 'detected_brand' in bas_df.columns:\n",
    "        print(\"\\nBAS brands:\")\n",
    "        bas_brands = bas_df['detected_brand'].value_counts()\n",
    "        for brand, count in bas_brands.items():\n",
    "            print(f\"  {brand:<20} {count:>6,}\")\n",
    "\n",
    "    # BAS protocols\n",
    "    bas_protocols = bas_df['eacs_reason'].str.extract(r'protocol:(\\w+)', expand=False).value_counts()\n",
    "    if len(bas_protocols) > 0:\n",
    "        print(\"\\nBAS protocols:\")\n",
    "        for protocol, count in bas_protocols.items():\n",
    "            if pd.notna(protocol):\n",
    "                print(f\"  {protocol:<20} {count:>6,}\")\n",
    "\n",
    "    print(\"=\"*70)"
   ],
   "id": "64f485142640b2ae",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-function Device Analysis",
   "id": "feb03fe0bce9169a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:48.533845Z",
     "start_time": "2026-01-05T13:50:48.522922600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 12: MULTI-FUNCTION DEVICE ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "if len(multi_function_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-FUNCTION DEVICE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nMulti-function devices: {len(multi_function_df):,}\")\n",
    "\n",
    "    # Category combinations\n",
    "    print(\"\\nCategory combinations:\")\n",
    "    for idx, row in multi_function_df.head(10).iterrows():\n",
    "        ip = row.get('ip', 'N/A')\n",
    "        categories = []\n",
    "        if row.get('is_eacs', False):\n",
    "            categories.append(f\"EACS({row.get('eacs_confidence', 0):.0f}%)\")\n",
    "        if row.get('is_vss', False):\n",
    "            categories.append(f\"VSS({row.get('vss_confidence', 0):.0f}%)\")\n",
    "        if row.get('is_ihas', False):\n",
    "            categories.append(f\"IHAS({row.get('ihas_confidence', 0):.0f}%)\")\n",
    "\n",
    "        brand = row.get('detected_brand', 'Unknown')\n",
    "        print(f\"  {ip:<20} {brand:<20} {' + '.join(categories)}\")\n",
    "\n",
    "    if len(multi_function_df) > 10:\n",
    "        print(f\"  ... and {len(multi_function_df) - 10} more\")\n",
    "\n",
    "    print(\"=\"*70)"
   ],
   "id": "d12c762a25e0feca",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CSV Exports",
   "id": "ed5be7a1f4183b63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:50:49.696252700Z",
     "start_time": "2026-01-05T13:50:48.535861100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 9: CSV EXPORTS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING CSV FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define export columns (if they exist)\n",
    "# Define comprehensive export columns for ISO assessment\n",
    "export_columns_base = [\n",
    "# =================================================================\n",
    "    # CORE IDENTIFICATION\n",
    "# =================================================================\n",
    "    'ip',\n",
    "    'asn',\n",
    "    'asn.number',\n",
    "    'asn.org',\n",
    "    'fqdns',\n",
    "    'fqdns_count',\n",
    "    'nidv_company',\n",
    "    'nidv_hit',\n",
    "    'scan_date',\n",
    "    'is_anycast',\n",
    "    \n",
    "# =================================================================\n",
    "    # SERVICE BASICS\n",
    "# =================================================================\n",
    "    'service.port',\n",
    "    'service.protocol',\n",
    "    'service.transport',\n",
    "    'service.banner',\n",
    "    'service.banner_hex',\n",
    "    'service.banner_sha1',\n",
    "    'service.banner_sha256',\n",
    "    'service.scanned_at',\n",
    "    'source_file',\n",
    "    'source_ip',\n",
    "    \n",
    "# =================================================================\n",
    "    # HTTP FIELDS (CRITICAL FOR A.5.15, A.8.2, A.8.3, A.8.5, A.8.26)\n",
    "# =================================================================\n",
    "    'service.http',\n",
    "    'service.http.title',\n",
    "    'service.http.headers',\n",
    "    'service.http.headers_sha256',\n",
    "    'service.http.body',\n",
    "    'service.http.body_sha1',\n",
    "    'service.http.body_sha256',\n",
    "    'service.http.status_code',\n",
    "    'service.http.favicon',\n",
    "    \n",
    "# =================================================================\n",
    "    # CVE/VULNERABILITY DATA (CRITICAL FOR A.8.8)\n",
    "# =================================================================\n",
    "    'service.cves',\n",
    "    'service.cves_count',\n",
    "    \n",
    "# =================================================================\n",
    "    # SERVICE FINGERPRINTS (FOR DETECTION & A.5.19)\n",
    "# =================================================================\n",
    "    'service.fingerprints.os',\n",
    "    'service.fingerprints.os.product',\n",
    "    'service.fingerprints.os.version',\n",
    "    'service.fingerprints.os.arch',\n",
    "    'service.fingerprints.service',\n",
    "    'service.fingerprints.service.product',\n",
    "    'service.fingerprints.service.version',\n",
    "    'service.fingerprints.tags',\n",
    "    'service.fingerprints.tags_count',\n",
    "    'service.fingerprints.technologies',\n",
    "    'service.fingerprints.technologies_count',\n",
    "    \n",
    "# =================================================================\n",
    "    # TLS/SSL DATA (CRITICAL FOR A.8.24)\n",
    "# =================================================================\n",
    "    'service.tls',\n",
    "    'service.tls.supported_versions',\n",
    "    'service.tls.supported_versions_count',\n",
    "    'service.tls.is_self_signed',\n",
    "    'service.tls.is_valid',\n",
    "    'service.tls.is_trusted',\n",
    "    'service.tls.expires_at',\n",
    "    'service.tls.valid_from',\n",
    "    'service.tls.validity_seconds',\n",
    "    'service.tls.issuer',\n",
    "    'service.tls.issuer.common_name',\n",
    "    'service.tls.issuer.organization',\n",
    "    'service.tls.issuer.country',\n",
    "    'service.tls.subject',\n",
    "    'service.tls.subject.common_name',\n",
    "    'service.tls.subject.organization',\n",
    "    'service.tls.subject.country',\n",
    "    'service.tls.fingerprint_sha1',\n",
    "    'service.tls.fingerprint_sha256',\n",
    "    'service.tls.serial_number',\n",
    "    'service.tls.jarm',\n",
    "    \n",
    "# =================================================================\n",
    "    # SSH DATA (FOR A.8.5, A.8.20)\n",
    "# =================================================================\n",
    "    'service.ssh',\n",
    "    'service.ssh.server_id',\n",
    "    'service.ssh.hassh',\n",
    "    \n",
    "# =================================================================\n",
    "    # GEOGRAPHY\n",
    "# =================================================================\n",
    "    'geo.city_name',\n",
    "    'geo.country_iso_code',\n",
    "    'geo.country_name',\n",
    "]\n",
    "\n",
    "def export_cpss_category(df_subset, device_type, confidence_col, reason_col):\n",
    "    \"\"\"Export CPSS category to CSV with confidence scores\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        print(f\"\\n⚠️  No {device_type} devices to export\")\n",
    "        return\n",
    "\n",
    "    # Find available columns\n",
    "    export_cols = [col for col in export_columns_base if col in df_subset.columns]\n",
    "\n",
    "    # Add CPSS-specific columns\n",
    "    export_cols.extend([confidence_col, reason_col])\n",
    "\n",
    "    if 'detected_brand' in df_subset.columns:\n",
    "        export_cols.append('detected_brand')\n",
    "\n",
    "    # Add BAS flag for EACS\n",
    "    if device_type == 'EACS' and 'is_bas' in df_subset.columns:\n",
    "        export_cols.append('is_bas')\n",
    "\n",
    "    # Add multi-function flag\n",
    "    if 'is_multi_function' in df_subset.columns:\n",
    "        export_cols.append('is_multi_function')\n",
    "\n",
    "    # Export\n",
    "    filename = f'cpss_{device_type.lower()}_services_enhanced.csv'\n",
    "    filepath = output_dir / filename\n",
    "    df_subset[export_cols].to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"\\n {device_type} exported: {filename}\")\n",
    "    print(f\"  Services: {len(df_subset):,}\")\n",
    "    print(f\"  Avg confidence: {df_subset[confidence_col].mean():.1f}%\")\n",
    "    print(f\"  Columns: {len(export_cols)}\")\n",
    "\n",
    "    # High confidence sample\n",
    "    high_conf = df_subset[df_subset[confidence_col] >= 90]\n",
    "    if len(high_conf) > 0:\n",
    "        print(f\"\\n  High confidence sample (≥90%, first 3):\")\n",
    "        for idx, row in high_conf.head(3).iterrows():\n",
    "            ip = row.get('ip', 'N/A')\n",
    "            port = row.get('service.port', 'N/A')\n",
    "            brand = row.get('detected_brand', 'N/A')\n",
    "            conf = row.get(confidence_col, 0)\n",
    "            reason = str(row.get(reason_col, ''))[:40]\n",
    "            print(f\"    {ip}:{port} | {brand:<15} | {conf:>3.0f}% | {reason}\")\n",
    "\n",
    "# Export each category\n",
    "\n",
    "ihas_df = calculate_is_kev(ihas_df)\n",
    "vss_df = calculate_is_kev(vss_df)\n",
    "eacs_df = calculate_is_kev(eacs_df)\n",
    "# Calculate KEV flags\n",
    "\n",
    "export_cpss_category(eacs_df, 'EACS', 'eacs_confidence', 'eacs_reason')\n",
    "export_cpss_category(vss_df, 'VSS', 'vss_confidence', 'vss_reason')\n",
    "export_cpss_category(ihas_df, 'IHAS', 'ihas_confidence', 'ihas_reason')\n",
    "\n",
    "# Export multi-function devices\n",
    "if len(multi_function_df) > 0:\n",
    "    multi_export_cols = [col for col in export_columns_base if col in multi_function_df.columns]\n",
    "    multi_export_cols.extend(['is_eacs', 'is_vss', 'is_ihas',\n",
    "                              'eacs_confidence', 'vss_confidence', 'ihas_confidence',\n",
    "                              'eacs_reason', 'vss_reason', 'ihas_reason'])\n",
    "\n",
    "    filepath = output_dir / 'cpss_multi_function_devices.csv'\n",
    "    multi_function_df[multi_export_cols].to_csv(filepath, index=False)\n",
    "    print(f\"\\n Multi-function devices exported: cpss_multi_function_devices.csv\")\n",
    "    print(f\"  Services: {len(multi_function_df):,}\")\n",
    "\n",
    "# Export combined CPSS dataset\n",
    "all_cpss = pd.concat([\n",
    "    eacs_df.assign(cpss_primary_category='EACS'),\n",
    "    vss_df.assign(cpss_primary_category='VSS'),\n",
    "    ihas_df.assign(cpss_primary_category='IHAS')\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(all_cpss) > 0:\n",
    "    export_cols = [col for col in export_columns_base if col in all_cpss.columns]\n",
    "    export_cols.extend(['cpss_primary_category',\n",
    "                       'eacs_confidence', 'vss_confidence', 'ihas_confidence',\n",
    "                       'eacs_reason', 'vss_reason', 'ihas_reason'])\n",
    "\n",
    "    if 'detected_brand' in all_cpss.columns:\n",
    "        export_cols.append('detected_brand')\n",
    "    if 'is_bas' in all_cpss.columns:\n",
    "        export_cols.append('is_bas')\n",
    "    if 'is_multi_function' in all_cpss.columns:\n",
    "        export_cols.append('is_multi_function')\n",
    "\n",
    "\n",
    "    filepath = output_dir / 'cpss_all_services_enhanced.csv'\n",
    "    # Ensure is_kev is in export columns\n",
    "    if \"is_kev\" not in export_cols:\n",
    "        export_cols.append(\"is_kev\")\n",
    "    \n",
    "    # Fill missing is_kev values with False\n",
    "    if \"is_kev\" in all_cpss.columns:\n",
    "        all_cpss[\"is_kev\"] = all_cpss[\"is_kev\"].fillna(False)\n",
    "    else:\n",
    "        all_cpss[\"is_kev\"] = False\n",
    "    all_cpss[[col for col in export_cols if col in all_cpss.columns]].to_csv(filepath, index=False)\n",
    "    print(f\"\\n Combined CPSS exported: cpss_all_services_enhanced.csv\")\n",
    "    print(f\"  Total services: {len(all_cpss):,}\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "7666c7d1d3e7e999",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPORTING CSV FILES\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'service.cves'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3811\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:167\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:196\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 'service.cves'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 161\u001B[39m\n\u001B[32m    157\u001B[39m             \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m    \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mip\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mport\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbrand\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m<15\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconf\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m>3.0f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m% | \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreason\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    159\u001B[39m \u001B[38;5;66;03m# Export each category\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m161\u001B[39m ihas_df = \u001B[43mcalculate_is_kev\u001B[49m\u001B[43m(\u001B[49m\u001B[43mihas_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    162\u001B[39m vss_df = calculate_is_kev(vss_df)\n\u001B[32m    163\u001B[39m eacs_df = calculate_is_kev(eacs_df)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 34\u001B[39m, in \u001B[36mcalculate_is_kev\u001B[39m\u001B[34m(df)\u001B[39m\n\u001B[32m     28\u001B[39m     \u001B[38;5;66;03m# Method 2: Check for known KEV CVEs (would need KEV list)\u001B[39;00m\n\u001B[32m     29\u001B[39m     \u001B[38;5;66;03m# For now, we'll assume MODAT data already marks KEVs\u001B[39;00m\n\u001B[32m     30\u001B[39m     \u001B[38;5;66;03m# If not, this function should be enhanced with actual KEV list\u001B[39;00m\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mis_kev\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mservice.cves\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m.apply(check_kev)\n\u001B[32m     36\u001B[39m kev_count = df[\u001B[33m'\u001B[39m\u001B[33mis_kev\u001B[39m\u001B[33m'\u001B[39m].sum()\n\u001B[32m     37\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  KEV calculation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkev_count\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m services with Known Exploited Vulnerabilities\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py:4107\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4105\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.nlevels > \u001B[32m1\u001B[39m:\n\u001B[32m   4106\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n\u001B[32m-> \u001B[39m\u001B[32m4107\u001B[39m indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4108\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[32m   4109\u001B[39m     indexer = [indexer]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3814\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3815\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3816\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3817\u001B[39m     ):\n\u001B[32m   3818\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3819\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3820\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3821\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3822\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3823\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[32m   3824\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'service.cves'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisations",
   "id": "96828471f0da7edf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 10: VISUALIZATIONS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. CPSS Category Distribution (Pie)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "cpss_counts = pd.Series({\n",
    "    'EACS': len(eacs_df),\n",
    "    'VSS': len(vss_df),\n",
    "    'IHAS': len(ihas_df)\n",
    "})\n",
    "colors_pie = [COLORS['eacs'], COLORS['vss'], COLORS['ihas']]\n",
    "wedges, texts, autotexts = ax1.pie(cpss_counts.values, labels=cpss_counts.index,\n",
    "                                     autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(12)\n",
    "ax1.set_title('CPSS Category Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# 2. Services by Category (Bar)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars = ax2.bar(cpss_counts.index, cpss_counts.values, color=colors_pie,\n",
    "               edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Number of Services', fontsize=11)\n",
    "ax2.set_title('CPSS Services by Category', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for bar, count in zip(bars, cpss_counts.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, count,\n",
    "            f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 3. Confidence Score Distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "confidence_data = []\n",
    "confidence_labels = []\n",
    "if len(eacs_df) > 0:\n",
    "    confidence_data.append(eacs_df['eacs_confidence'].values)\n",
    "    confidence_labels.append('EACS')\n",
    "if len(vss_df) > 0:\n",
    "    confidence_data.append(vss_df['vss_confidence'].values)\n",
    "    confidence_labels.append('VSS')\n",
    "if len(ihas_df) > 0:\n",
    "    confidence_data.append(ihas_df['ihas_confidence'].values)\n",
    "    confidence_labels.append('IHAS')\n",
    "\n",
    "if confidence_data:\n",
    "    bp = ax3.boxplot(confidence_data, labels=confidence_labels, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors_pie[:len(confidence_data)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    ax3.set_ylabel('Confidence Score (%)', fontsize=11)\n",
    "    ax3.set_title('Confidence Score Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    ax3.set_ylim(0, 105)\n",
    "\n",
    "# 4. Detection Method Comparison\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "detection_methods = {\n",
    "    'Protocol': [],\n",
    "    'HTTP Path': [],\n",
    "    'Brand/Product': [],\n",
    "    'Tag': [],\n",
    "    'Keyword': []\n",
    "}\n",
    "\n",
    "for df_subset, reason_col, label in [(eacs_df, 'eacs_reason', 'EACS'),\n",
    "                                      (vss_df, 'vss_reason', 'VSS'),\n",
    "                                      (ihas_df, 'ihas_reason', 'IHAS')]:\n",
    "    if len(df_subset) > 0:\n",
    "        total = len(df_subset)\n",
    "        detection_methods['Protocol'].append(len(df_subset[df_subset[reason_col].str.contains('protocol:', na=False)]) / total * 100)\n",
    "        detection_methods['HTTP Path'].append(len(df_subset[df_subset[reason_col].str.contains('http_path:', na=False)]) / total * 100)\n",
    "        detection_methods['Brand/Product'].append(len(df_subset[df_subset[reason_col].str.contains('brand:|product:|model:', na=False)]) / total * 100)\n",
    "        detection_methods['Tag'].append(len(df_subset[df_subset[reason_col].str.contains('tag:', na=False)]) / total * 100)\n",
    "        detection_methods['Keyword'].append(len(df_subset[df_subset[reason_col].str.contains('keyword', na=False)]) / total * 100)\n",
    "    else:\n",
    "        for key in detection_methods:\n",
    "            detection_methods[key].append(0)\n",
    "\n",
    "x = np.arange(len(['EACS', 'VSS', 'IHAS']))\n",
    "width = 0.15\n",
    "multiplier = 0\n",
    "\n",
    "for method, values in detection_methods.items():\n",
    "    offset = width * multiplier\n",
    "    bars = ax4.bar(x + offset, values, width, label=method)\n",
    "    multiplier += 1\n",
    "\n",
    "ax4.set_ylabel('Percentage (%)', fontsize=11)\n",
    "ax4.set_title('Detection Method Comparison by Category', fontsize=14, fontweight='bold', pad=15)\n",
    "ax4.set_xticks(x + width * 2)\n",
    "ax4.set_xticklabels(['EACS', 'VSS', 'IHAS'])\n",
    "ax4.legend(loc='upper right', fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Top Brands (Combined)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "all_brands = []\n",
    "if len(eacs_df) > 0 and 'detected_brand' in eacs_df.columns:\n",
    "    all_brands.extend(eacs_df['detected_brand'].tolist())\n",
    "if len(vss_df) > 0 and 'detected_brand' in vss_df.columns:\n",
    "    all_brands.extend(vss_df['detected_brand'].tolist())\n",
    "if len(ihas_df) > 0 and 'detected_brand' in ihas_df.columns:\n",
    "    all_brands.extend(ihas_df['detected_brand'].tolist())\n",
    "\n",
    "if all_brands:\n",
    "    brand_counts = pd.Series(all_brands).value_counts().head(15)\n",
    "    ax5.barh(range(len(brand_counts)), brand_counts.values, color=COLORS['quaternary'])\n",
    "    ax5.set_yticks(range(len(brand_counts)))\n",
    "    ax5.set_yticklabels(brand_counts.index)\n",
    "    ax5.set_xlabel('Number of Services', fontsize=11)\n",
    "    ax5.set_title('Top 15 CPSS Brands (All Categories)', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    for i, (brand, count) in enumerate(brand_counts.items()):\n",
    "        ax5.text(count, i, f' {count:,}', va='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Enhanced CPSS Identification Summary', fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.savefig(output_dir / 'cpss_identification_enhanced_summary.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization saved: cpss_identification_enhanced_summary.jpg\")\n",
    "print(\"=\"*70)"
   ],
   "id": "4a85816f59bf0028",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summary Report",
   "id": "7176aee32a6020f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# CELL 13: SUMMARY REPORT\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "ENHANCED CPSS IDENTIFICATION SUMMARY REPORT\n",
    "{'='*70}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "DATASET OVERVIEW\n",
    "{'-'*70}\n",
    "Total services analyzed:          {len(df):>12,}\n",
    "\n",
    "CPSS IDENTIFICATION RESULTS\n",
    "{'-'*70}\n",
    "EACS (Access Control):            {len(eacs_df):>12,} ({len(eacs_df)/len(df)*100:>6.2f}%)\n",
    "  └─ BAS subcategory:             {eacs_df['is_bas'].sum():>12,} ({eacs_df['is_bas'].sum()/len(eacs_df)*100 if len(eacs_df)>0 else 0:>6.2f}% of EACS)\n",
    "  └─ Average confidence:          {eacs_df['eacs_confidence'].mean() if len(eacs_df)>0 else 0:>12.1f}%\n",
    "\n",
    "VSS (Video Surveillance):         {len(vss_df):>12,} ({len(vss_df)/len(df)*100:>6.2f}%)\n",
    "  └─ Average confidence:          {vss_df['vss_confidence'].mean() if len(vss_df)>0 else 0:>12.1f}%\n",
    "\n",
    "IHAS (Intrusion & Alarm):        {len(ihas_df):>12,} ({len(ihas_df)/len(df)*100:>6.2f}%)\n",
    "  └─ Average confidence:          {ihas_df['ihas_confidence'].mean() if len(ihas_df)>0 else 0:>12.1f}%\n",
    "\n",
    "Total CPSS devices:               {total_cpss:>12,} ({total_cpss/len(df)*100:>6.2f}%)\n",
    "Multi-function devices:           {len(multi_function_df):>12,}\n",
    "Non-CPSS services:                {len(df)-total_cpss:>12,} ({(len(df)-total_cpss)/len(df)*100:>6.2f}%)\n",
    "\n",
    "ENHANCEMENT FEATURES APPLIED\n",
    "{'-'*70}\n",
    " Protocol detection (RTSP, ONVIF, BACnet, SIA DC-09, Contact ID)\n",
    " HTTP path pattern matching\n",
    " Model number detection\n",
    " Confidence scoring system (0-100%)\n",
    " Multi-function device handling\n",
    " BAS subcategory flagging\n",
    " Enhanced cloud provider exclusions\n",
    "\n",
    "TOP DETECTION METHODS\n",
    "{'-'*70}\n",
    "EACS:  {eacs_df['eacs_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(eacs_df)>0 else 'N/A':<30} {eacs_df['eacs_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(eacs_df)>0 else 0:>6,}\n",
    "VSS:   {vss_df['vss_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(vss_df)>0 else 'N/A':<30} {vss_df['vss_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(vss_df)>0 else 0:>6,}\n",
    "IHAS: {ihas_df['ihas_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).index[0] if len(ihas_df)>0 else 'N/A':<30} {ihas_df['ihas_reason'].str.extract(r'^(\\w+):', expand=False).value_counts().head(1).values[0] if len(ihas_df)>0 else 0:>6,}\n",
    "\n",
    "FILES GENERATED\n",
    "{'-'*70}\n",
    "cpss_eacs_services_enhanced.csv   {len(eacs_df):>12,} services\n",
    "cpss_vss_services_enhanced.csv    {len(vss_df):>12,} services\n",
    "cpss_ihas_services_enhanced.csv   {len(ihas_df):>12,} services\n",
    "cpss_all_services_enhanced.csv    {total_cpss:>12,} services\n",
    "cpss_multi_function_devices.csv   {len(multi_function_df):>12,} services\n",
    "cpss_identification_enhanced_summary.jpg\n",
    "\n",
    "NEXT STEPS\n",
    "{'-'*70}\n",
    "1. Validate CSV exports\n",
    "2. Review high-confidence matches (≥90%)\n",
    "3. Investigate multi-function devices\n",
    "4. Proceed to ISO controls assessment (analyses_3)\n",
    "\n",
    "{'='*70}\n",
    "End of Report\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save report\n",
    "report_path = output_dir / 'cpss_identification_enhanced_report.txt'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\n Summary report saved: cpss_identification_enhanced_report.txt\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENHANCED CPSS IDENTIFICATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n All files saved to: {output_dir.absolute()}\")\n",
    "print(\"\\nEnhancement Summary:\")\n",
    "print(f\"  • {len(eacs_df):,} EACS devices (avg {eacs_df['eacs_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {len(vss_df):,} VSS devices (avg {vss_df['vss_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {len(ihas_df):,} IHAS devices (avg {ihas_df['ihas_confidence'].mean():.1f}% confidence)\")\n",
    "print(f\"  • {total_cpss:,} total CPSS devices identified\")\n",
    "print(\"\\nReview the CSV files and proceed to ISO control assessment!\")\n",
    "print(\"=\"*70)"
   ],
   "id": "4411768948df8701",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
